{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "56267f9c-db63-4bc4-b79a-838fa87ab267",
   "metadata": {
    "tags": []
   },
   "source": [
    "## üåü **Overview**\n",
    "In this project, we‚Äôll build a Neural Network to predict the **sale price** of houses based on several key features. Using this predictive model, we‚Äôll dive into the relationships between house characteristics and their impact on price. To evaluate our model‚Äôs performance, we‚Äôll use essential regression metrics such as **Mean Absolute Error (MAE)**, **Mean Squared Error (MSE)**, and **R-squared (R¬≤)**. Additionally, we‚Äôll visualize the Neural Network structure to understand how it processes and learns from the data.\n",
    "\n",
    "---\n",
    "\n",
    "### üìä **Dataset Description**\n",
    "Our dataset contains rich information about houses, with various attributes that influence pricing:\n",
    "\n",
    "- **price** üí≤: Final sale price of the house (Target Variable)\n",
    "- **area** üìê: Total area of the house in square feet\n",
    "- **bedrooms** üõèÔ∏è: Number of bedrooms\n",
    "- **bathrooms** üõÅ: Number of bathrooms\n",
    "- **stories** üè¢: Number of stories (floors)\n",
    "- **mainroad** üöó: Whether the house is located on the main road (`yes`/`no`)\n",
    "- **guestroom** üõãÔ∏è: Indicates if there‚Äôs a guest room in the house (`yes`/`no`)\n",
    "- **basement** ‚¨áÔ∏è: Whether the house has a basement (`yes`/`no`)\n",
    "- **hotwaterheating** üî•: Availability of hot water heating (`yes`/`no`)\n",
    "- **airconditioning** ‚ùÑÔ∏è: Indicates if there is air conditioning (`yes`/`no`)\n",
    "- **parking** üöô: Number of parking spaces available\n",
    "- **prefarea** üå≥: Whether the house is located in a preferred area (`yes`/`no`)\n",
    "- **furnishingstatus** üõ†Ô∏è: Furnishing status of the house (e.g., furnished, semi-furnished, unfurnished)\n",
    "\n",
    "---\n",
    "\n",
    "### üéØ **Objective**\n",
    "Our objective is to use a Neural Network model to predict the **price** of a house using the provided features. We aim to achieve:\n",
    "\n",
    "- **Accurate Predictions**: Maximize the model‚Äôs performance in predicting house prices.\n",
    "- **Feature Understanding**: Gain insights into how each feature impacts the sale price.\n",
    "- **Model Evaluation**: Use regression metrics to ensure that the model generalizes well.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fa452e9-2986-4336-beaa-d37fb38f1b97",
   "metadata": {
    "tags": []
   },
   "source": [
    "\n",
    "## ‚ú® Data Cleaning and Imputation for Missing Values ‚ú®\n",
    "\n",
    "In this notebook, we will handle missing data from a housing dataset. Specifically, we will identify missing values and impute them using an appropriate strategy (mode imputation for the `hotwaterheating` column).\n",
    "\n",
    "---\n",
    "\n",
    "## üìù 1. Load the Dataset\n",
    "\n",
    "Let's begin by loading the dataset and displaying a few records to get an overview of the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efac0230-2703-47b3-a8c1-7be580d9c604",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "file_path = 'Housing_V0.csv'\n",
    "housing_data = pd.read_csv(file_path)\n",
    "\n",
    "# Display the first few rows of the dataset\n",
    "housing_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b7d3158-80bf-4bbc-b446-4d961e0d55d5",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## üîç 2. Check for Missing Values\n",
    "\n",
    "Before performing any data imputation, it's essential to check for missing values in the dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d30e7d27-10eb-433f-8654-eb6abcac9b98",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Check for any missing values\n",
    "null_values = housing_data.isnull().sum()\n",
    "\n",
    "# Display the columns with null values\n",
    "null_values[null_values > 0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "211b9010-b72e-4f4c-8e0f-bcca06d786b3",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üîß 3. Impute Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a274b7c-6d3c-4bd8-835f-eacecd18ec64",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Impute missing values using the mode of 'hotwaterheating'\n",
    "# Calculate the mode for 'hotwaterheating'\n",
    "mode_value = housing_data['hotwaterheating'].mode()[0]\n",
    "\n",
    "# Assign back to the column without chaining\n",
    "housing_data['hotwaterheating'] = housing_data['hotwaterheating'].fillna(mode_value)\n",
    "\n",
    "# Verify if any missing values remain\n",
    "null_values_after_imputation = housing_data.isnull().sum()\n",
    "\n",
    "# Display the result\n",
    "null_values_after_imputation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8f30955-a735-4f7a-a157-49e8e78abfb7",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "## üö´ 4. Removing Outliers Based on Price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a6104ff-5aa4-4b7d-82ce-dc3ad873b25e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Calculate Q1, Q3, and IQR for 'price'\n",
    "Q1 = housing_data['price'].quantile(0.25)\n",
    "Q3 = housing_data['price'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "# Define the upper and lower bounds for outliers\n",
    "lower_bound = Q1 - 1.5 * IQR\n",
    "upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "# Identify rows with outliers in the 'price' column\n",
    "outliers = housing_data[(housing_data['price'] < lower_bound) | (housing_data['price'] > upper_bound)]\n",
    "\n",
    "# Remove outliers from the dataset\n",
    "housing_data_cleaned = housing_data[~((housing_data['price'] < lower_bound) | (housing_data['price'] > upper_bound))]\n",
    "\n",
    "# Verify the number of remaining rows\n",
    "housing_data_cleaned.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f7a2637",
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_data_cleaned"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ad10cdf-374d-4286-b07a-f18e3b8e2393",
   "metadata": {},
   "source": [
    "## üîÑ5. One-Hot Encoding the 'Furnishingstatus' Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9098040b-2140-4c24-af27-0ab47458a78d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "housing_data = pd.get_dummies(housing_data_cleaned, columns=['furnishingstatus'], drop_first=True)\n",
    "# Assuming 'df' is your DataFrame\n",
    "\n",
    "housing_data = housing_data.replace({'yes': 1, 'no': 0})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7e6c52d-36f6-4ac2-ba59-79f741cefb88",
   "metadata": {
    "tags": []
   },
   "source": [
    "## üìè6. Scaling the 'price' Column Between 0 and 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2aef409-27f4-458a-ab54-ac5dd1ab52e1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Initialize the MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Scale the 'area' column\n",
    "housing_data[['price']] = scaler.fit_transform(housing_data[['price']])\n",
    "\n",
    "# Display the first few rows of the scaled dataset\n",
    "housing_data.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d12411ee-3306-44be-8977-a68b06cc26c1",
   "metadata": {},
   "source": [
    "## ‚úÇÔ∏è7. Splitting the Dataset into Training and Testing Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55af0717-30e7-4784-8357-18c96ac1fab0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Define the feature set and target variable\n",
    "X = housing_data.drop(columns=['price'])  # Example: using 'price' as the target variable\n",
    "y = housing_data['price']\n",
    "\n",
    "# Split the dataset (80% training, 20% testing)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Display the shapes of the resulting datasets\n",
    "print(\"Training set shape (X_train, y_train):\", X_train.shape, y_train.shape)\n",
    "print(\"Testing set shape (X_test, y_test):\", X_test.shape, y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c2da467-9d98-473d-9924-d79630dd2370",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1631e76b-e2f7-4159-9fc9-aa62a6b737d0",
   "metadata": {
    "tags": []
   },
   "source": [
    "## üìè8. Scaling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8738c665-d3c4-4f74-9d8c-0178e776abe7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "# Initialize the StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit the scaler on the training data and transform both training and testing sets\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Now X_train_scaled and X_test_scaled are scaled versions of the original feature sets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21d7386b-1089-4dcf-9967-7edb183f3025",
   "metadata": {
    "tags": []
   },
   "source": [
    "\n",
    "# üè† **Understanding Neural Networks with the Housing Dataset**\n",
    "\n",
    "## **Overview:**\n",
    "In this document, we will explore how neural networks work using the Housing dataset. After preprocessing the data, we will:\n",
    "1. Train a Multi-Layer Perceptron (MLP) model to predict the sale price of a house.\n",
    "2. Understand the important parameters of MLP and how they affect model performance.\n",
    "3. Evaluate model performance using regression metrics like Mean Absolute Error (MAE), Mean Squared Error (MSE), and R-squared.\n",
    "4. Dive deeper into how **hidden layers** and **activation functions** contribute to the model's learning ability.\n",
    "\n",
    "---\n",
    "\n",
    "## **üóÉÔ∏è Dataset Description:**\n",
    "The Housing dataset contains features such as area, number of bedrooms and bathrooms, presence of a basement, and other relevant attributes that help determine the sale price of a house. The target variable is **'price'**, representing the sale price of the house.\n",
    "\n",
    "---\n",
    "\n",
    "## **üîÑ Preprocessing the Data:**\n",
    "1. **Scaling**: Standardize features to ensure better model performance.\n",
    "2. **Train-Test Split**: Divide data into training and testing sets (e.g., 80%-20%).\n",
    "\n",
    "---\n",
    "\n",
    "## **ü§ñ How Neural Networks Work:**\n",
    "\n",
    "Multi-Layer Perceptrons (MLP) are a type of feedforward neural network, composed of an input layer, one or more hidden layers, and an output layer. Here‚Äôs how they work:\n",
    "\n",
    "### 1. **Layers and Neurons**:\n",
    "   - **Input Layer**: Receives input data (features) and passes it to the next layer.\n",
    "   - **Hidden Layers**: Contain neurons that learn features from the input data. The number of neurons and layers determines the model's capacity to learn complex patterns.\n",
    "   - **Output Layer**: Provides the final output, which, in this case, is a continuous prediction (house price).\n",
    "\n",
    "### 2. **Activation Functions**:\n",
    "   - Activation functions introduce **non-linearity** into the model, allowing it to learn complex relationships. Common activation functions include **ReLU** (Rectified Linear Unit) and **sigmoid**.\n",
    "\n",
    "### 3. **Training and Backpropagation**:\n",
    "   - **Forward Pass**: The input data passes through the network, and predictions are generated.\n",
    "   - **Loss Calculation**: A loss function (e.g., **Mean Squared Error**) calculates the error between predicted and actual values.\n",
    "   - **Backpropagation**: The model adjusts the weights by minimizing the loss using an optimization algorithm (e.g., **stochastic gradient descent**).\n",
    "\n",
    "### 4. **Reduce Overfitting**\n",
    "   - **Regularization (`alpha`)**: Tune `alpha` for L2 regularization to penalize large weights.\n",
    "   - **Reduce Model Complexity**: Adjust model depth and neuron counts to control complexity.\n",
    "   - **Cross-Validation**: Apply cross-validation to find optimal parameters (like `hidden_layer_sizes` and `alpha`).\n",
    "\n",
    "---\n",
    "\n",
    "## **üìâ Plotting Training vs. Testing Curves**\n",
    "\n",
    "To assess the model‚Äôs learning dynamics and detect overfitting or underfitting, we plot the training and testing loss curves over multiple epochs:\n",
    "- **Training Curve**: Tracks the model‚Äôs performance on the training data.\n",
    "- **Testing Curve**: Shows how well the model generalizes to unseen data.\n",
    "\n",
    "This plot helps identify points where overfitting may begin.\n",
    "\n",
    "---\n",
    "\n",
    "## **üîç Interpreting with SHAP**\n",
    "\n",
    "### **What are SHAP Values?**\n",
    "SHAP (SHapley Additive exPlanations) values help interpret the contribution of each feature in the model's prediction. SHAP values indicate how much each feature pushes a prediction higher or lower compared to the average prediction.\n",
    "\n",
    "### **Using SHAP with Our Model**\n",
    "1. **SHAP Summary Plot**: Shows feature importance by displaying the average SHAP value of each feature.\n",
    "2. **SHAP Force Plot**: Visualizes individual predictions, highlighting the positive or negative contribution of each feature.\n",
    "3. **SHAP Dependence Plot**: Examines the effect of individual features in the context of interactions with other features.\n",
    "\n",
    "---\n",
    "\n",
    "## **üìä Evaluation Metrics**\n",
    "\n",
    "To evaluate the model, we use:\n",
    "- **Mean Absolute Error (MAE)**: Measures the average magnitude of errors in predictions.\n",
    "- **Mean Squared Error (MSE)**: Penalizes larger errors, useful for gauging prediction accuracy.\n",
    "- **R-squared (R¬≤)**: Indicates the proportion of variance explained by the model.\n",
    "\n",
    "---\n",
    "\n",
    "## **üéØ Summary and Insights**\n",
    "By combining scaling, overfitting reduction, visualizations, and SHAP values, we gain a comprehensive understanding of the model‚Äôs performance and feature importance. This project not only predicts house prices but also provides valuable insights into the impact of each feature, making it useful for real estate valuation and analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b383d592-3c09-4b2d-93a0-f82f8226af22",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define the MLPRegressor\n",
    "def our_mlp(mlp, num_epochs):\n",
    "    # Prepare lists to track the loss\n",
    "    train_losses = []\n",
    "    test_losses = []\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        # Train with partial fit (incremental learning)\n",
    "        mlp.partial_fit(X_train, y_train)\n",
    "\n",
    "        # Calculate training loss (MSE for regression)\n",
    "        train_loss = mean_squared_error(y_train, mlp.predict(X_train))\n",
    "        train_losses.append(train_loss)\n",
    "\n",
    "        # Calculate test loss (MSE for regression)\n",
    "        test_loss = mean_squared_error(y_test, mlp.predict(X_test))\n",
    "        test_losses.append(test_loss)\n",
    "\n",
    "        if epoch % 100 == 0:\n",
    "            print(f\"Epoch {epoch+1}/{num_epochs} - Training Loss: {train_loss:.4f}, Test Loss: {test_loss:.4f}\")\n",
    "    return train_losses, test_losses\n",
    "    \n",
    "def plot_mlp(train_losses, test_losses):\n",
    "    # Plot the training and test loss curves\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(range(1, len(train_losses) + 1), train_losses, '-', color='r', label='Training loss (MSE)')\n",
    "    plt.plot(range(1, len(test_losses) + 1), test_losses, '-', color='g', label='Test loss (MSE)')\n",
    "    plt.title('Training and Test Loss Curves (MLPRegressor)')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Mean Squared Error')\n",
    "    plt.legend(loc='best')\n",
    "    plt.grid()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95b01cec-95fa-4c43-b3b6-8428792eb44f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mlp = MLPRegressor(hidden_layer_sizes=(50, 30), max_iter=1000, warm_start=True, random_state=42)\n",
    "(train_losses, test_losses) = our_mlp(mlp, 1000)\n",
    "plot_mlp(train_losses, test_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "facd7e7b-38f8-4797-9459-552fa66e42ed",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Explanation\n",
    "- **Hidden Layers**: We define two hidden layers with 50 and 30 neurons, respectively. More neurons and layers can increase the model's ability to learn but may also lead to overfitting.\n",
    "- **Max Iterations**: Controls how many times the model will iterate to optimize weights.\n",
    "- **Activation Function**: The default activation function is **ReLU** for hidden layers, allowing the model to handle non-linearity in data.\n",
    "\n",
    "---\n",
    "\n",
    "## **üìä Evaluating Model Performance:**\n",
    "\n",
    "To evaluate the MLP model, we use several metrics:\n",
    "\n",
    "1. **Accuracy**: Measures the overall correctness of predictions.\n",
    "2. **Classification Report**: Provides **precision**, **recall**, and **F1-score** for each class, offering a detailed view of model performance.\n",
    "3. **Confusion Matrix**: Shows the counts of **true positives**, **true negatives**, **false positives**, and **false negatives**, helping us understand where the model makes mistakes.\n",
    "\n",
    "---\n",
    "\n",
    "## **üí° Insights on Neural Network Performance:**\n",
    "\n",
    "- **Hidden Layer Size**: Adding more neurons or layers can improve performance but may also require more computational resources and may lead to overfitting if not properly regularized.\n",
    "- **Scaling**: Neural networks are sensitive to input scale, hence scaling is crucial for effective training.\n",
    "\n",
    "In this document, we explored how neural networks can be applied to predict MBA admissions. We went through data preprocessing, model training, and performance evaluation, offering insights into how MLP parameters affect model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a555dbc-442d-4788-8750-72849b5c7ae8",
   "metadata": {
    "tags": []
   },
   "source": [
    "# üåü Reducing Overfitting in MLPClassifier: A Comprehensive Guide\n",
    "\n",
    "When training an `MLPClassifier`, overfitting can hinder performance on unseen data. This guide explores various strategies to mitigate overfitting effectively. Let's dive into each method and make your model robust and generalizable! üéâ \n",
    "\n",
    "---\n",
    "\n",
    "## üõ†Ô∏è 1. Regularization with `alpha`\n",
    "\n",
    "- **What It Does**: The `alpha` parameter controls L2 regularization, which penalizes large weights, reducing model complexity.\n",
    "- **How to Use It**:\n",
    "  ```python\n",
    "\n",
    "  ```\n",
    "- **Pro Tip**: Start with a small `alpha` (e.g., 0.0001) and increase it gradually. Too high an `alpha` can lead to underfitting. üéõÔ∏è"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "582e4b8b-4a95-4466-973b-edf7363e485a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "mlp = MLPRegressor(hidden_layer_sizes=(50, 30), max_iter=10000, alpha=0.0001, random_state=42)\n",
    "(train_losses, test_losses) = our_mlp(mlp, 1000)\n",
    "plot_mlp(train_losses, test_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5098f195-649f-41ac-b51f-804e3fda453e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "mlp = MLPRegressor(hidden_layer_sizes=(50, 30), max_iter=10000, alpha=0.1, random_state=42)\n",
    "(train_losses, test_losses) = our_mlp(mlp, 1000)\n",
    "plot_mlp(train_losses, test_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31c63fa4-bfd2-42c4-a15b-9fc67491c87c",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üîç 2. Reduce Model Complexity\n",
    "\n",
    "- **Why**: Smaller networks have fewer parameters, making it harder for the model to memorize the training data.\n",
    "- **Example**:\n",
    "  ```python\n",
    "  mlp = MLPClassifier(hidden_layer_sizes=(30,), max_iter=300, random_state=42)\n",
    "  ```\n",
    "- **Configurations to Try**: Test hidden layer sizes like `(50,)`, `(30, 20)`, or `(50, 50)`. Finding the balance is key! ‚öñÔ∏è"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e04fbd9-8912-4f17-ad40-85fc80cd638a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mlp = MLPRegressor(hidden_layer_sizes=(10), max_iter=10000, alpha=0.1, random_state=42)\n",
    "(train_losses, test_losses) = our_mlp(mlp, 1000)\n",
    "plot_mlp(train_losses, test_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa5e46c5-9d58-4913-bcbb-b16839de3ff0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mlp = MLPRegressor(hidden_layer_sizes=(20), max_iter=1000, alpha=0.1, random_state=42)\n",
    "(train_losses, test_losses) = our_mlp(mlp, 1000)\n",
    "plot_mlp(train_losses, test_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76ba43e0-5a45-4771-a043-03ab7b2aad09",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## üéØ 3. Cross-Validation\n",
    "\n",
    "- **Purpose**: Use cross-validation to tune hyperparameters, such as `hidden_layer_sizes`, `alpha`, and `learning_rate_init`.\n",
    "- **Implementation**:\n",
    "\n",
    "- **Note**: Cross-validation provides a robust evaluation of model performance and helps find the best configuration.\n",
    "\n",
    "\n",
    "\n",
    "Applying one or more of these strategies can help mitigate overfitting and improve the generalization of an `MLPRegressor`. Experimenting with combinations of these techniques often yields the best results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9f60691-4583-4b7e-82ff-0f88e043fbcd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {\n",
    "  'hidden_layer_sizes': [(10,),  (30, 10), (50, 20), (100, 30), (300, 30)],\n",
    "  'alpha': [0.1,  0.01, 0.001]\n",
    "}\n",
    "grid_search = GridSearchCV(MLPRegressor(max_iter=1000, random_state=42), param_grid, cv=5)\n",
    "grid_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc1877cd-5d8c-4ec7-84fe-d9416b9f5b3a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mlp = MLPRegressor(hidden_layer_sizes=(300, 30), max_iter=1000, alpha=0.1, random_state=42)\n",
    "(train_losses, test_losses) = our_mlp(mlp, 1000)\n",
    "plot_mlp(train_losses, test_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0742e779-ad56-42fc-a74b-9ec2a2dd50e7",
   "metadata": {
    "tags": []
   },
   "source": [
    "\n",
    "# üåü Understanding SHAP Values: A Quick Guide\n",
    "\n",
    "SHAP (SHapley Additive exPlanations) values are a powerful tool to interpret machine learning models. They help us see how much each feature contributes to a specific prediction, making even complex models more understandable.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚ú® Key Concepts\n",
    "\n",
    "- **SHAP Value**: Measures each feature's contribution to a prediction.\n",
    "  - **Positive SHAP value** ‚û°Ô∏è The feature pushes the prediction higher.\n",
    "  - **Negative SHAP value** ‚û°Ô∏è The feature pushes the prediction lower.\n",
    "\n",
    "### üîç How SHAP Values Work\n",
    "\n",
    "1. **Base Value** üéØ: This is the average prediction of the model across all data points, serving as the starting point for each prediction.\n",
    "2. **Feature Contributions** üìä: SHAP values show how much each feature ‚Äúpushes‚Äù the prediction away from the base value, calculated by considering all possible combinations of features.\n",
    "\n",
    "---\n",
    "\n",
    "### üí° Example\n",
    "\n",
    "Imagine a model predicts a credit score of **0.8** (on a scale of 0 to 1), and the base value is **0.5**:\n",
    "\n",
    "- **Salary** ü§ë with a SHAP value of **0.2** means it **increases** the score by 0.2.\n",
    "- **Debt** üí∏ with a SHAP value of **-0.1** means it **decreases** the score by 0.1.\n",
    "\n",
    "So, the final prediction **0.8** is the base value **0.5** plus the contributions from salary and debt.\n",
    "\n",
    "---\n",
    "\n",
    "### üìù Summary\n",
    "\n",
    "SHAP values let us open the ‚Äúblack box‚Äù of machine learning models, revealing how each feature influences the outcome. They provide transparency and insight, helping you trust and understand your model's predictions.\n",
    "\n",
    "---\n",
    "\n",
    "With SHAP values, even the most complex models become interpretable, empowering you to make informed decisions with confidence! üéâ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47adacdd-086d-45dc-bc22-8a70b9baf7d3",
   "metadata": {},
   "source": [
    "# üîé Using SHAP Values for Model Interpretation in MLPClassifier\n",
    "\n",
    "SHAP (SHapley Additive exPlanations) values provide insights into feature contributions to model predictions, which is especially helpful in understanding and refining models trained with scikit-learn's `MLPClassifier`.\n",
    "\n",
    "## Why Use SHAP?\n",
    "- **Feature Importance**: SHAP values help interpret which features are driving the model‚Äôs predictions and can reveal if specific features overly influence the model.\n",
    "- **Overfitting Detection**: If certain features dominate the predictions, it might indicate overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89204dd1-0351-40d3-a3f6-22c3cd75236a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import shap\n",
    "# Explain the model's predictions using SHAP\n",
    "\n",
    "# Randomly sample 100 instances from the training set for the SHAP background\n",
    "background = shap.sample(X_train, 20)\n",
    "\n",
    "explainer = shap.KernelExplainer(mlp.predict, background, feature_names=X.columns)\n",
    "shap_values = explainer(X_test)  # Calculate SHAP values for the test set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c08fc44-55ce-4a40-aa47-258dea65b755",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Understanding SHAP Plots for Model Interpretation\n",
    "\n",
    "Using SHAP values provides insights into feature importance and individual predictions. This document explains how to interpret different types of SHAP plots, including summary, force, and dependence plots.\n",
    "\n",
    "---\n",
    "\n",
    "## 1. SHAP Summary Plot (Bar)\n",
    "\n",
    "```python\n",
    "# Summary plot of SHAP values (bar plot for feature importance)\n",
    "shap.summary_plot(shap_values, X_test, plot_type=\"bar\")\n",
    "```\n",
    "### What It Shows\n",
    "- **Purpose**: Provides a high-level view of feature importance across the entire dataset.\n",
    "- **Interpretation**: Each bar represents the average absolute SHAP value of a feature, indicating its overall importance in the model.\n",
    "- **Insight**: Longer bars indicate features that have a larger impact on model predictions on average. This is useful for identifying the most influential features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fbd7c7f-33ac-4b09-95b8-a8090d1dc995",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "shap.summary_plot(shap_values, X_test, plot_type=\"bar\", feature_names=X.columns)  # Bar plot for feature importance\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "739579aa-e813-4e12-bc30-57f59000ddd4",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "\n",
    "## 2. SHAP Summary Plot (Density)\n",
    "\n",
    "```python\n",
    "\n",
    "```\n",
    "### What It Shows\n",
    "- **Purpose**: Displays the distribution of SHAP values for each feature, highlighting how they contribute to different predictions.\n",
    "- **Interpretation**: Each point represents a SHAP value for a feature in a specific instance. Points are color-coded by feature values (e.g., blue for low and red for high).\n",
    "- **Insight**: The spread of SHAP values for each feature shows its influence across various instances. Features with both positive and negative SHAP values indicate they contribute to increasing or decreasing the prediction based on their values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52995a60-3450-4a0d-ab2e-2f5009406dd6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Detailed SHAP value plot for individual features (traditional summary plot)\n",
    "shap.summary_plot(shap_values, X_test, feature_names=X.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b1ee432-d5f6-4a7c-964f-90e10576e9f5",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "\n",
    "## 3. SHAP Force Plot (Single Prediction)\n",
    "\n",
    "\n",
    "### What It Shows\n",
    "- **Purpose**: Explains a single prediction by showing how each feature value pushes the prediction away from the expected value (baseline).\n",
    "- **Interpretation**: Features that push the prediction higher are shown in red, while those pushing it lower are in blue. The length of each segment represents the strength of the feature‚Äôs impact.\n",
    "- **Insight**: The force plot helps in understanding the main drivers behind a specific prediction. It is particularly useful in identifying key factors that lead to higher or lower model outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96001d55-280e-4379-976e-ceb02446f281",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Round SHAP values and the expected value to 2 decimals\n",
    "rounded_shap_values = np.round(shap_values[0].values, 2)\n",
    "rounded_expected_value = np.round(explainer.expected_value, 2)\n",
    "\n",
    "# Convert the first instance of X_test to a pandas Series and round values\n",
    "X_test_rounded = pd.Series(X_test[0], index=X.columns).apply(lambda x: f\"{x:.2f}\")\n",
    "\n",
    "# Generate the force plot\n",
    "shap.force_plot(rounded_expected_value, rounded_shap_values, X_test_rounded, \n",
    "                matplotlib=True, feature_names=X.columns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b149227-b3e1-4fce-8041-80271429bf62",
   "metadata": {},
   "source": [
    "# SHAP Force Plot Explanation\n",
    "\n",
    "This document provides a brief explanation of the SHAP force plot and how each feature affects the model's prediction for a single instance.\n",
    "\n",
    "---\n",
    "\n",
    "### Structure of the Plot\n",
    "\n",
    "1. **Base Value**: This is the starting point (or baseline) of the model‚Äôs prediction, representing the average prediction across all training data. In this plot, the base value is approximately **0.25**.\n",
    "\n",
    "2. **f(x)**: This represents the final prediction for this specific instance after considering the contributions of each feature. Here, `f(x)` is **0.34**.\n",
    "\n",
    "3. **Feature Contributions (Arrows)**:\n",
    "   - **Red Arrows**: Features that push the prediction higher (positive impact on `f(x)`).\n",
    "   - **Blue Arrows**: Features that push the prediction lower (negative impact on `f(x)`).\n",
    "   - The length of each arrow represents the strength of the feature‚Äôs impact on the prediction.\n",
    "\n",
    "---\n",
    "\n",
    "### Interpretation of Each Feature\n",
    "\n",
    "- **`bedrooms = 1.39`**: This feature positively impacts `f(x)`, increasing it by around 0.09.\n",
    "- **`parking = 0.37`**: Adds positively to the prediction, pushing it up by approximately 0.04.\n",
    "- **`area = 0.34`**: Also contributes positively, with an impact around 0.03.\n",
    "- **`basement = 1.34`**: Slightly increases `f(x)`, with an impact close to 0.05.\n",
    "- **`bathrooms = 1.54`**: Has a significant positive effect, increasing `f(x)` by approximately 0.07.\n",
    "\n",
    "These features collectively increase the model's prediction.\n",
    "\n",
    "- **`mainroad = -2.46`**: This feature has the largest negative impact, reducing `f(x)` by about 0.09.\n",
    "- **`prefarea = -0.5`**: Decreases `f(x)` slightly, with an impact close to 0.02.\n",
    "- **`hotwaterheating = -0.28`**: Reduces the prediction by approximately 0.01.\n",
    "- **`airconditioning = -0.67`**: Has a small negative effect, lowering `f(x)` by around 0.04.\n",
    "\n",
    "These features collectively decrease the model's prediction.\n",
    "\n",
    "### Summary\n",
    "\n",
    "The model‚Äôs final prediction of **0.34** is the combined result of all these feature contributions. Positive contributions from features such as `bedrooms`, `parking`, `area`, `basement`, and `bathrooms` are partially balanced by negative contributions from `mainroad`, `prefarea`, `hotwaterheating`, and `airconditioning`.\n",
    "\n",
    "This plot provides insight into which features have the strongest influence on this prediction and whether their impact is positive or negative, helping to explain the model's decision for this particular instance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8996208-6215-4fed-b5f8-6add61f22c12",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. SHAP Dependence Plot (for a specific feature)\n",
    "\n",
    "### What It Shows\n",
    "- **Purpose**: Shows the effect of a specific feature on the prediction while considering the impact of other interacting features.\n",
    "- **Interpretation**: The x-axis represents values of the selected feature, and the y-axis shows SHAP values for that feature. Color coding represents another interacting feature, providing context on how interactions affect predictions.\n",
    "- **Insight**: This plot highlights both the individual impact of a feature and any interactions with other features. For example, if 'gmat' score has a high SHAP value at certain levels, the plot can reveal how it influences predictions in the context of other variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a81a21ca-3376-4955-9d1f-c789ebbc55c3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Interactive SHAP dependence plot for a specific feature (e.g., 'gmat' score)\n",
    "shap.dependence_plot('bedrooms', shap_values.values, X_test, feature_names=X.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f25c66b8-fafd-4eae-a40a-e4e8909e7ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.plots.scatter(shap_values[:, \"bedrooms\"], color=shap_values[:, \"bathrooms\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3994900",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b1f133b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
