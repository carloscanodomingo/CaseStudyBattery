{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"Synthetic_data/experiment_results.csv\"\n",
    "df = pd.read_csv(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iteration_columns = df.filter(regex='^Iteration').copy()\n",
    "# Convertir a numérico\n",
    "iteration_columns = iteration_columns.apply(pd.to_numeric, errors='coerce')\n",
    "# Convertir todos los ceros en NaN\n",
    "iteration_columns = iteration_columns.replace(0, np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "# Reduce the number of batteries to a smaller subset for visualization (e.g., first 50 batteries)\n",
    "subset_data = iteration_columns.head(2000)\n",
    "\n",
    "# Create a DataFrame to store the iteration values in a long format for the subset\n",
    "long_data = pd.DataFrame()\n",
    "\n",
    "for index, row in subset_data.iterrows():\n",
    "    valid_values = row[row != np.nan]  # Remove padding values (0)\n",
    "    temp_df = pd.DataFrame({\n",
    "        'Battery_ID': index,\n",
    "        'Iteration': np.arange(len(valid_values)),\n",
    "        'SoH': valid_values.values\n",
    "    })\n",
    "    long_data = pd.concat([long_data, temp_df], ignore_index=True)\n",
    "\n",
    "# Plotting with Plotly Express for the subset\n",
    "fig = px.line(long_data, x='Iteration', y='SoH', color='Battery_ID', title='State of Health (SoH) Values for 50 Batteries')\n",
    "fig.update_layout(showlegend=False, xaxis=dict(range=[0, 500]), yaxis=dict(range=[3, 4.3]))  # Hide legend and limit y-axis range\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate SoH\n",
    "soh = iteration_columns.div(iteration_columns.iloc[:, 0], axis=0)\n",
    "soh_thresholds = pd.DataFrame()\n",
    "# Create columns for the iteration in which SoH reaches 0.98, 0.95, 0.9, 0.85, and 0.8\n",
    "thresholds = [0.98, 0.95, 0.9, 0.85, 0.8]\n",
    "for threshold in thresholds:\n",
    "    soh_thresholds[f'{threshold}'] = soh.apply(lambda row: next((i for i, v in enumerate(row) if v <= threshold), np.nan), axis=1)\n",
    "\n",
    "soh_thresholds.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soh_thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soh\n",
    "# Create a new dataframe with the specified columns and the SoH thresholds\n",
    "columns_of_interest = ['charge_c_rate_modulation', 'protocol_choice_prob', 'charge_soc_modulation', 'rest_duration_modulation', 'discharge_factor_modulation', 'discharge_soc_modulation']\n",
    "df_with_thresholds = df[columns_of_interest].copy()\n",
    "\n",
    "# Add the SoH thresholds to the dataframe\n",
    "for threshold in thresholds:\n",
    "    df_with_thresholds[f'{threshold}'] = soh_thresholds[f'{threshold}']\n",
    "\n",
    "# Create separate dataframes for each threshold and remove NaN values\n",
    "dfs_by_threshold = {}\n",
    "for threshold in thresholds:\n",
    "    df_threshold = df_with_thresholds[['charge_c_rate_modulation', 'protocol_choice_prob', 'charge_soc_modulation', 'rest_duration_modulation', 'discharge_factor_modulation', 'discharge_soc_modulation', f'{threshold}']].dropna()\n",
    "    dfs_by_threshold[f'{threshold}'] = df_threshold\n",
    "\n",
    "# Example: Access the dataframe for SoH 0.8\n",
    "dfs_by_threshold['0.8']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs_by_threshold.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install xgboost shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "import shap\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "# Initialize a dictionary to store the most important feature, metrics, and SHAP values for each threshold\n",
    "most_important_features = {}\n",
    "metrics = {}\n",
    "shap_values_dict = {}\n",
    "\n",
    "# Loop through each threshold and train an XGBoost model\n",
    "for threshold in thresholds:\n",
    "    # Prepare the data\n",
    "    df_threshold = dfs_by_threshold[f'{threshold}']\n",
    "    X = df_threshold[columns_of_interest]\n",
    "    y = df_threshold[f'{threshold}']\n",
    "    \n",
    "    # Train the XGBoost model\n",
    "    model = xgb.XGBRegressor()\n",
    "    model.fit(X, y)\n",
    "    \n",
    "    # Calculate SHAP values\n",
    "    explainer = shap.Explainer(model)\n",
    "    shap_values = explainer(X)\n",
    "    shap_values_dict[f'{threshold}'] = shap_values\n",
    "    \n",
    "    # Determine the most important feature\n",
    "    shap_importance = np.abs(shap_values.values).mean(axis=0)\n",
    "    most_important_feature = columns_of_interest[np.argmax(shap_importance)]\n",
    "    most_important_features[f'{threshold}'] = most_important_feature\n",
    "    \n",
    "    # Predict the values\n",
    "    y_pred = model.predict(X)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    mae = mean_absolute_error(y, y_pred)\n",
    "    mse = mean_squared_error(y, y_pred)\n",
    "    r2 = r2_score(y, y_pred)\n",
    "    metrics[f'{threshold}'] = {'MAE': mae, 'MSE': mse, 'R²': r2}\n",
    "    \n",
    "    # Print the most important feature and metrics for each threshold\n",
    "    print(f\"The most important feature for SoH {threshold} is {most_important_feature}\")\n",
    "    print(f\"Metrics for SoH {threshold}: MAE = {mae}, MSE = {mse}, R² = {r2}\")\n",
    "    \n",
    "    # Plot the SHAP values for the current threshold\n",
    "    shap.summary_plot(shap_values, X, feature_names=columns_of_interest)\n",
    "    shap.dependence_plot(most_important_feature, shap_values.values, X, feature_names=columns_of_interest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.initjs()\n",
    "shap.force_plot(explainer.expected_value, shap_values.values, X, feature_names=columns_of_interest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install mlflow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import mlflow.xgboost\n",
    "import shap\n",
    "import xgboost as xgb\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import pickle\n",
    "import warnings\n",
    "\n",
    "# Suppress XGBoost model format warning\n",
    "warnings.filterwarnings(action='ignore', category=UserWarning, module='xgboost')\n",
    "\n",
    "# Initialize MLflow experiment\n",
    "mlflow.set_experiment(\"New SoH Prediction Experiment\")\n",
    "\n",
    "# Start an MLflow run\n",
    "with mlflow.start_run():\n",
    "    # Log parameters\n",
    "    mlflow.log_param(\"data_path\", path)\n",
    "    mlflow.log_param(\"thresholds\", thresholds)\n",
    "    \n",
    "    # Initialize a dictionary to store the most important feature, metrics, and SHAP values for each threshold\n",
    "    most_important_features = {}\n",
    "    metrics = {}\n",
    "    shap_values_dict = {}\n",
    "\n",
    "    # Loop through each threshold and train an XGBoost model\n",
    "    for threshold in thresholds:\n",
    "        # Prepare the data\n",
    "        df_threshold = dfs_by_threshold[f'{threshold}']\n",
    "        X = df_threshold[columns_of_interest]\n",
    "        y = df_threshold[f'{threshold}']\n",
    "        \n",
    "        # Train the XGBoost model\n",
    "        model = xgb.XGBRegressor()\n",
    "        model.fit(X, y)\n",
    "        \n",
    "        # Calculate SHAP values\n",
    "        explainer = shap.Explainer(model)\n",
    "        shap_values = explainer(X)\n",
    "        shap_values_dict[f'{threshold}'] = shap_values\n",
    "        \n",
    "        # Determine the most important feature\n",
    "        shap_importance = np.abs(shap_values.values).mean(axis=0)\n",
    "        most_important_feature = columns_of_interest[np.argmax(shap_importance)]\n",
    "        most_important_features[f'{threshold}'] = most_important_feature\n",
    "        \n",
    "        # Predict the values\n",
    "        y_pred = model.predict(X)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        mae = mean_absolute_error(y, y_pred)\n",
    "        mse = mean_squared_error(y, y_pred)\n",
    "        r2 = r2_score(y, y_pred)\n",
    "        metrics[f'{threshold}'] = {'MAE': mae, 'MSE': mse, 'R²': r2}\n",
    "        \n",
    "        # Log metrics\n",
    "        # Log the model with input example\n",
    "        input_example = X.iloc[:5]\n",
    "        mlflow.xgboost.log_model(model, f\"model_{threshold}\", input_example=input_example)\n",
    "        mlflow.log_metric(f\"R2_{threshold}\", r2)\n",
    "        \n",
    "        # Log the model\n",
    "        mlflow.xgboost.log_model(model, f\"model_{threshold}\")\n",
    "        \n",
    "        # Print the most important feature and metrics for each threshold\n",
    "        print(f\"The most important feature for SoH {threshold} is {most_important_feature}\")\n",
    "        print(f\"Metrics for SoH {threshold}: MAE = {mae}, MSE = {mse}, R² = {r2}\")\n",
    "        \n",
    "        # Plot the SHAP values for the current threshold\n",
    "        shap.summary_plot(shap_values, X, feature_names=columns_of_interest)\n",
    "        shap.dependence_plot(most_important_feature, shap_values.values, X, feature_names=columns_of_interest)\n",
    "    \n",
    "    # Log the most important features\n",
    "    mlflow.log_dict(most_important_features, \"most_important_features.json\")\n",
    "    \n",
    "    # Log the SHAP values\n",
    "    for threshold, shap_values in shap_values_dict.items():\n",
    "        shap_values_file = f\"shap_values_{threshold}.pkl\"\n",
    "        with open(shap_values_file, \"wb\") as f:\n",
    "            pickle.dump(shap_values, f)\n",
    "        mlflow.log_artifact(shap_values_file)\n",
    "\n",
    "    # Log the experimental setup\n",
    "    mlflow.log_param(\"experimental_setup\", {\n",
    "        \"data_path\": path,\n",
    "        \"columns_of_interest\": columns_of_interest,\n",
    "        \"thresholds\": thresholds\n",
    "    })\n",
    "\n",
    "    # Log the preprocessing steps\n",
    "    mlflow.log_param(\"preprocessing_steps\", {\n",
    "        \"iteration_columns_conversion\": \"Converted to numeric and replaced zeros with NaN\",\n",
    "        \"soh_calculation\": \"Calculated SoH and created thresholds\",\n",
    "        \"dataframe_creation\": \"Created dataframes for each threshold and removed NaN values\"\n",
    "    })\n",
    "\n",
    "    # Log the model training and evaluation details\n",
    "    mlflow.log_param(\"model_training\", {\n",
    "        \"model_type\": \"XGBoost\",\n",
    "        \"evaluation_metrics\": [\"MAE\", \"MSE\", \"R²\"]\n",
    "    })\n",
    "\n",
    "    # Log the most important features\n",
    "    mlflow.log_dict(most_important_features, \"most_important_features.json\")\n",
    "\n",
    "    # Log the SHAP values\n",
    "    for threshold, shap_values in shap_values_dict.items():\n",
    "        shap_values_file = f\"shap_values_{threshold}.pkl\"\n",
    "        with open(shap_values_file, \"wb\") as f:\n",
    "            pickle.dump(shap_values, f)\n",
    "        mlflow.log_artifact(shap_values_file)\n",
    "\n",
    "    # Track the specific file of the experimental result\n",
    "    # This will log the experimental result file to MLflow\n",
    "    mlflow.log_artifact(path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import mlflow.xgboost\n",
    "import mlflow.data\n",
    "import shap\n",
    "import xgboost as xgb\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pickle\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Suppress XGBoost model format warning\n",
    "warnings.filterwarnings(action='ignore', category=UserWarning, module='xgboost')\n",
    "\n",
    "# Initialize MLflow experiment\n",
    "mlflow.set_experiment(\"New SoH Prediction Experiment\")\n",
    "\n",
    "# Start an MLflow run\n",
    "with mlflow.start_run() as parent_run:\n",
    "    # Log parameters\n",
    "    path = \"Synthetic_data/experiment_results.csv\"\n",
    "    mlflow.log_param(\"data_path\", path)\n",
    "    \n",
    "    # Load data\n",
    "    df = pd.read_csv(path)\n",
    "    mlflow.log_param(\"data_shape\", df.shape)\n",
    "    \n",
    "    # Preprocessing steps\n",
    "    iteration_columns = df.filter(regex='^Iteration').copy()\n",
    "    mlflow.log_param(\"iteration_columns_shape\", iteration_columns.shape)\n",
    "    \n",
    "    # Convert to numeric\n",
    "    iteration_columns = iteration_columns.apply(pd.to_numeric, errors='coerce')\n",
    "    mlflow.log_param(\"iteration_columns_conversion\", \"Converted to numeric\")\n",
    "    \n",
    "    # Replace zeros with NaN\n",
    "    iteration_columns = iteration_columns.replace(0, np.nan)\n",
    "    mlflow.log_param(\"iteration_columns_replace_zeros\", \"Replaced zeros with NaN\")\n",
    "    \n",
    "    # Calculate SoH\n",
    "    soh = iteration_columns.div(iteration_columns.iloc[:, 0], axis=0)\n",
    "    mlflow.log_param(\"soh_calculation\", \"Calculated SoH\")\n",
    "    \n",
    "    # Create SoH thresholds\n",
    "    soh_thresholds = pd.DataFrame()\n",
    "    thresholds = [0.98, 0.95, 0.9, 0.85, 0.8]\n",
    "    for threshold in thresholds:\n",
    "        soh_thresholds[str(threshold)] = soh.apply(\n",
    "            lambda row: next((i for i, v in enumerate(row) if v <= threshold), len(row)),\n",
    "            axis=1\n",
    "        )\n",
    "    soh_thresholds = soh_thresholds.astype(float)\n",
    "    mlflow.log_param(\"soh_thresholds_creation\", \"Created SoH thresholds\")\n",
    "    \n",
    "    # Log the number of NaN values in SoH thresholds\n",
    "    mlflow.log_param(\"soh_thresholds_nan_count\", soh_thresholds.isna().sum().to_dict())\n",
    "    \n",
    "    # Add the SoH thresholds to the dataframe\n",
    "    columns_of_interest = [\n",
    "        'charge_c_rate_modulation', 'protocol_choice_prob', 'charge_soc_modulation',\n",
    "        'rest_duration_modulation', 'discharge_factor_modulation', 'discharge_soc_modulation'\n",
    "    ]\n",
    "    df_with_thresholds = df[columns_of_interest].copy()\n",
    "    for threshold in thresholds:\n",
    "        df_with_thresholds[str(threshold)] = soh_thresholds[str(threshold)]\n",
    "    mlflow.log_param(\"df_with_thresholds_shape\", df_with_thresholds.shape)\n",
    "    \n",
    "    # Create separate dataframes for each threshold and remove NaN values\n",
    "    dfs_by_threshold = {}\n",
    "    for threshold in thresholds:\n",
    "        df_threshold = df_with_thresholds[columns_of_interest + [str(threshold)]].dropna()\n",
    "        dfs_by_threshold[f'{threshold}'] = df_threshold\n",
    "    \n",
    "    def process_threshold(threshold, df_threshold):\n",
    "        with mlflow.start_run(nested=True) as child_run:\n",
    "            # Prepare the data\n",
    "            X = df_threshold[columns_of_interest]\n",
    "            y = df_threshold[f'{threshold}']\n",
    "            \n",
    "            # Split data into train and test sets\n",
    "            X_train, X_test, y_train, y_test = train_test_split(\n",
    "                X, y, test_size=0.2, random_state=42\n",
    "            )\n",
    "            \n",
    "            # Log the training Dataset\n",
    "            train_data = X_train.copy()\n",
    "            train_data['label'] = y_train\n",
    "            train_dataset = mlflow.data.from_pandas(train_data, targets='label')\n",
    "            mlflow.log_input(train_dataset, context='training')\n",
    "            \n",
    "            # Train the XGBoost model\n",
    "            model = xgb.XGBRegressor()\n",
    "            model.fit(X_train, y_train)\n",
    "            \n",
    "            # Log the model\n",
    "            mlflow.xgboost.log_model(\n",
    "                model, artifact_path=f\"model_{threshold}\", input_example=X_test\n",
    "            )\n",
    "            \n",
    "            # Predict the values\n",
    "            y_pred = model.predict(X_test)\n",
    "            \n",
    "            # Build the evaluation dataset\n",
    "            eval_data = X_test.copy()\n",
    "            eval_data['label'] = y_test\n",
    "            eval_data['predictions'] = y_pred  # Include predictions\n",
    "            \n",
    "            # Create the PandasDataset for use in mlflow evaluate\n",
    "            pd_dataset = mlflow.data.from_pandas(\n",
    "                eval_data.drop(columns=['predictions']), targets='label'\n",
    "            )\n",
    "            \n",
    "            # Log the evaluation Dataset\n",
    "            mlflow.log_input(pd_dataset, context='evaluation')\n",
    "            \n",
    "            # Evaluate the model\n",
    "            model_uri = f\"runs:/{child_run.info.run_id}/model_{threshold}\"\n",
    "            result = mlflow.evaluate(\n",
    "                model=model_uri,\n",
    "                data=pd_dataset,\n",
    "                targets=None,\n",
    "                model_type='regressor',\n",
    "                evaluators='default'\n",
    "            )\n",
    "            \n",
    "            # Calculate metrics\n",
    "            mae = mean_absolute_error(y_test, y_pred)\n",
    "            mse = mean_squared_error(y_test, y_pred)\n",
    "            r2 = r2_score(y_test, y_pred)\n",
    "            \n",
    "            # Log metrics\n",
    "            mlflow.log_metric(\"MAE\", mae)\n",
    "            mlflow.log_metric(\"MSE\", mse)\n",
    "            mlflow.log_metric(\"R2\", r2)\n",
    "            \n",
    "            # Calculate SHAP values on the test data\n",
    "            explainer = shap.Explainer(model)\n",
    "            shap_values = explainer(X_test)\n",
    "            \n",
    "            # Determine the most important feature\n",
    "            shap_importance = np.abs(shap_values.values).mean(axis=0)\n",
    "            most_important_feature = columns_of_interest[np.argmax(shap_importance)]\n",
    "            \n",
    "            # Log the most important feature\n",
    "            mlflow.log_param(\"most_important_feature\", most_important_feature)\n",
    "            \n",
    "            # Print the most important feature and metrics\n",
    "            print(f\"The most important feature for SoH {threshold} is {most_important_feature}\")\n",
    "            print(f\"Metrics for SoH {threshold}: MAE = {mae}, MSE = {mse}, R² = {r2}\")\n",
    "            \n",
    "            # Plot the SHAP values for the current threshold\n",
    "            shap.summary_plot(shap_values, X_test, feature_names=columns_of_interest, show=False)\n",
    "            plt.savefig(f\"shap_summary_plot_{threshold}.png\", bbox_inches='tight')\n",
    "            mlflow.log_artifact(f\"shap_summary_plot_{threshold}.png\")\n",
    "            plt.clf()\n",
    "            \n",
    "            shap.dependence_plot(\n",
    "                most_important_feature, shap_values.values, X_test,\n",
    "                feature_names=columns_of_interest, show=False\n",
    "            )\n",
    "            plt.savefig(f\"shap_dependence_plot_{threshold}.png\", bbox_inches='tight')\n",
    "            mlflow.log_artifact(f\"shap_dependence_plot_{threshold}.png\")\n",
    "            plt.clf()\n",
    "            \n",
    "            # Save the SHAP values\n",
    "            shap_values_file = f\"shap_values_{threshold}.pkl\"\n",
    "            with open(shap_values_file, \"wb\") as f:\n",
    "                pickle.dump(shap_values, f)\n",
    "            mlflow.log_artifact(shap_values_file)\n",
    "\n",
    "    \n",
    "    # Loop through each threshold and process\n",
    "    for threshold in thresholds:\n",
    "        df_threshold = dfs_by_threshold[f'{threshold}']\n",
    "        process_threshold(threshold, df_threshold)\n",
    "    \n",
    "    # Log the experimental setup\n",
    "    mlflow.log_param(\"experimental_setup\", {\n",
    "        \"data_path\": path,\n",
    "        \"columns_of_interest\": columns_of_interest,\n",
    "        \"thresholds\": thresholds\n",
    "    })\n",
    "    \n",
    "    # Log the dataset used\n",
    "    mlflow.log_param(\"dataset_used\", path)\n",
    "    \n",
    "    # Track the specific file of the experimental result\n",
    "    mlflow.log_artifact(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U \"ray[data,train,tune,serve]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import mlflow.xgboost\n",
    "import shap\n",
    "import xgboost as xgb\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import pickle\n",
    "import warnings\n",
    "from ray import tune, train\n",
    "from ray.tune.schedulers import ASHAScheduler\n",
    "\n",
    "# Suppress XGBoost model format warning\n",
    "warnings.filterwarnings(action='ignore', category=UserWarning, module='xgboost')\n",
    "\n",
    "# Initialize MLflow experiment\n",
    "mlflow.set_experiment(\"New SoH Prediction Experiment\")\n",
    "\n",
    "# Define the search space for hyperparameters\n",
    "search_space = {\n",
    "    \"learning_rate\": tune.loguniform(0.01, 0.1),\n",
    "    \"max_depth\": tune.randint(3, 10),\n",
    "    \"min_child_weight\": tune.randint(1, 6),\n",
    "    \"subsample\": tune.uniform(0.5, 1.0),\n",
    "    \"colsample_bytree\": tune.uniform(0.5, 1.0)\n",
    "}\n",
    "\n",
    "# Define the training function\n",
    "def train_model(config, threshold, X, y):\n",
    "    import xgboost as xgb\n",
    "    from sklearn.metrics import mean_absolute_error\n",
    "    # No need to import tune again if already imported globally\n",
    "    model = xgb.XGBRegressor(\n",
    "        learning_rate=config[\"learning_rate\"],\n",
    "        max_depth=config[\"max_depth\"],\n",
    "        min_child_weight=config[\"min_child_weight\"],\n",
    "        subsample=config[\"subsample\"],\n",
    "        colsample_bytree=config[\"colsample_bytree\"]\n",
    "    )\n",
    "    model.fit(X, y)\n",
    "    y_pred = model.predict(X)\n",
    "    mae = mean_absolute_error(y, y_pred)\n",
    "    train.report({\"mae\":mae})\n",
    "\n",
    "# Define or load necessary variables before running the script\n",
    "# For example:\n",
    "# path = \"path_to_your_data.csv\"\n",
    "# thresholds = [0.8, 0.85, 0.9]\n",
    "# columns_of_interest = [\"feature1\", \"feature2\", \"feature3\"]\n",
    "# dfs_by_threshold = {\"0.8\": df1, \"0.85\": df2, \"0.9\": df3}\n",
    "\n",
    "# Start an MLflow run\n",
    "with mlflow.start_run():\n",
    "    # Log parameters\n",
    "    mlflow.log_param(\"data_path\", path)\n",
    "    mlflow.log_param(\"thresholds\", thresholds)\n",
    "    \n",
    "    # Initialize dictionaries to store results\n",
    "    most_important_features = {}\n",
    "    metrics = {}\n",
    "    shap_values_dict = {}\n",
    "\n",
    "    # Loop through each threshold and train an XGBoost model\n",
    "    for threshold in thresholds:\n",
    "        # Prepare the data\n",
    "        df_threshold = dfs_by_threshold[str(threshold)]\n",
    "        X = df_threshold[columns_of_interest]\n",
    "        y = df_threshold[str(threshold)]\n",
    "        \n",
    "        # Perform hyperparameter tuning with Ray Tune\n",
    "        scheduler = ASHAScheduler(\n",
    "            metric=\"mae\",\n",
    "            mode=\"min\",\n",
    "            max_t=10,\n",
    "            grace_period=1,\n",
    "            reduction_factor=2\n",
    "        )\n",
    "        \n",
    "        # Sample a subset of the data to reduce memory usage\n",
    "        X_sample = X.sample(frac=0.5, random_state=42)\n",
    "        y_sample = y.loc[X_sample.index]\n",
    "\n",
    "        analysis = tune.run(\n",
    "            tune.with_parameters(train_model, threshold=threshold, X=X_sample, y=y_sample),\n",
    "            config=search_space,\n",
    "            num_samples=10,\n",
    "            scheduler=scheduler,\n",
    "             max_concurrent_trials=1  # Limit the number of concurrent trials\n",
    "            # For newer versions of Ray, limit concurrency using ConcurrencyLimiter if needed\n",
    "        )\n",
    "        \n",
    "        # Get the best hyperparameters\n",
    "        best_config = analysis.get_best_config(metric=\"mae\", mode=\"min\")\n",
    "        \n",
    "        # Train the XGBoost model with the best hyperparameters\n",
    "        model = xgb.XGBRegressor(\n",
    "            learning_rate=best_config[\"learning_rate\"],\n",
    "            max_depth=best_config[\"max_depth\"],\n",
    "            min_child_weight=best_config[\"min_child_weight\"],\n",
    "            subsample=best_config[\"subsample\"],\n",
    "            colsample_bytree=best_config[\"colsample_bytree\"]\n",
    "        )\n",
    "        model.fit(X, y)\n",
    "        \n",
    "        # Calculate SHAP values\n",
    "        explainer = shap.Explainer(model)\n",
    "        shap_values = explainer(X)\n",
    "        shap_values_dict[str(threshold)] = shap_values\n",
    "        \n",
    "        # Determine the most important feature\n",
    "        shap_importance = np.abs(shap_values.values).mean(axis=0)\n",
    "        most_important_feature = columns_of_interest[np.argmax(shap_importance)]\n",
    "        most_important_features[str(threshold)] = most_important_feature\n",
    "        \n",
    "        # Predict the values\n",
    "        y_pred = model.predict(X)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        mae = mean_absolute_error(y, y_pred)\n",
    "        mse = mean_squared_error(y, y_pred)\n",
    "        r2 = r2_score(y, y_pred)\n",
    "        metrics[str(threshold)] = {'MAE': mae, 'MSE': mse, 'R²': r2}\n",
    "        \n",
    "        # Log metrics\n",
    "        # Log the model with input example\n",
    "        input_example = X.iloc[:5]\n",
    "        mlflow.xgboost.log_model(model, f\"model_{threshold}\", input_example=input_example)\n",
    "        mlflow.log_metric(f\"R2_{threshold}\", r2)\n",
    "        \n",
    "        # Print the most important feature and metrics for each threshold\n",
    "        print(f\"The most important feature for SoH {threshold} is {most_important_feature}\")\n",
    "        print(f\"Metrics for SoH {threshold}: MAE = {mae}, MSE = {mse}, R² = {r2}\")\n",
    "        \n",
    "        # Plot the SHAP values for the current threshold\n",
    "        # Save the plots instead of displaying them\n",
    "        shap.summary_plot(shap_values, X, feature_names=columns_of_interest, show=False)\n",
    "        plt.savefig(f\"shap_summary_{threshold}.png\")\n",
    "        mlflow.log_artifact(f\"shap_summary_{threshold}.png\")\n",
    "        plt.close()\n",
    "\n",
    "        shap.dependence_plot(most_important_feature, shap_values.values, X, feature_names=columns_of_interest, show=False)\n",
    "        plt.savefig(f\"shap_dependence_{threshold}.png\")\n",
    "        mlflow.log_artifact(f\"shap_dependence_{threshold}.png\")\n",
    "        plt.close()\n",
    "    \n",
    "    # Log the most important features\n",
    "    mlflow.log_dict(most_important_features, \"most_important_features.json\")\n",
    "    \n",
    "    # Log the SHAP values\n",
    "    for threshold, shap_values in shap_values_dict.items():\n",
    "        shap_values_file = f\"shap_values_{threshold}.pkl\"\n",
    "        with open(shap_values_file, \"wb\") as f:\n",
    "            pickle.dump(shap_values, f)\n",
    "        mlflow.log_artifact(shap_values_file)\n",
    "\n",
    "    # Log the experimental setup\n",
    "    mlflow.log_param(\"experimental_setup\", {\n",
    "        \"data_path\": path,\n",
    "        \"columns_of_interest\": columns_of_interest,\n",
    "        \"thresholds\": thresholds\n",
    "    })\n",
    "\n",
    "    # Log the preprocessing steps\n",
    "    mlflow.log_param(\"preprocessing_steps\", {\n",
    "        \"iteration_columns_conversion\": \"Converted to numeric and replaced zeros with NaN\",\n",
    "        \"soh_calculation\": \"Calculated SoH and created thresholds\",\n",
    "        \"dataframe_creation\": \"Created dataframes for each threshold and removed NaN values\"\n",
    "    })\n",
    "\n",
    "    # Log the model training and evaluation details\n",
    "    mlflow.log_param(\"model_training\", {\n",
    "        \"model_type\": \"XGBoost\",\n",
    "        \"evaluation_metrics\": [\"MAE\", \"MSE\", \"R²\"]\n",
    "    })\n",
    "\n",
    "    # Track the specific file of the experimental result\n",
    "    # This will log the experimental result file to MLflow\n",
    "    mlflow.log_artifact(path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# Suppress XGBoost model format warning\n",
    "warnings.filterwarnings(action='ignore', category=UserWarning, module='xgboost')\n",
    "\n",
    "# Initialize MLflow experiment\n",
    "mlflow.set_experiment(\"New SoH Prediction Experiment\")\n",
    "\n",
    "# Define the search space for hyperparameters, including 'threshold'\n",
    "search_space = {\n",
    "    \"learning_rate\": hyperopt.hp.loguniform(\"learning_rate\", np.log(0.01), np.log(0.1)),\n",
    "    \"max_depth\": hyperopt.hp.randint(\"max_depth\", 3, 10),\n",
    "    \"min_child_weight\": hyperopt.hp.randint(\"min_child_weight\", 1, 6),\n",
    "    \"subsample\": hyperopt.hp.uniform(\"subsample\", 0.5, 1.0),\n",
    "    \"colsample_bytree\": hyperopt.hp.uniform(\"colsample_bytree\", 0.5, 1.0),\n",
    "    \"threshold\": hyperopt.hp.choice(\"threshold\", [0.98, 0.95, 0.9, 0.85, 0.8])  # Threshold as a hyperparameter\n",
    "}\n",
    "\n",
    "# Define the training function\n",
    "def train_model(config, data, columns_of_interest):\n",
    "    import xgboost as xgb\n",
    "    from sklearn.metrics import r2_score\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "\n",
    "    # Extract the threshold from the config\n",
    "    threshold = config[\"threshold\"]\n",
    "    \n",
    "    # Preprocess data based on the threshold\n",
    "    # Calculate SoH (State of Health) based on the iteration columns\n",
    "    iteration_columns = data.filter(regex='^Iteration').copy()\n",
    "    iteration_columns = iteration_columns.apply(pd.to_numeric, errors='coerce')\n",
    "    iteration_columns = iteration_columns.replace(0, np.nan)\n",
    "    soh = iteration_columns.div(iteration_columns.iloc[:, 0], axis=0)\n",
    "    \n",
    "    # Calculate the target variable 'y' based on the threshold\n",
    "    soh_threshold = soh.apply(\n",
    "        lambda row: next((i for i, v in enumerate(row) if v <= threshold), len(row)),\n",
    "        axis=1\n",
    "    )\n",
    "    soh_threshold = soh_threshold.astype(float)\n",
    "    \n",
    "    # Combine features and target into a single DataFrame\n",
    "    df_with_threshold = data[columns_of_interest].copy()\n",
    "    df_with_threshold['target'] = soh_threshold\n",
    "    df_with_threshold = df_with_threshold.dropna()\n",
    "    \n",
    "    # Split features and target\n",
    "    X = df_with_threshold[columns_of_interest]\n",
    "    y = df_with_threshold['target']\n",
    "    \n",
    "    # Split data into training and validation sets\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42\n",
    "    )\n",
    "    \n",
    "    # Initialize the XGBoost regressor with hyperparameters from 'config'\n",
    "    model = xgb.XGBRegressor(\n",
    "        learning_rate=config[\"learning_rate\"],\n",
    "        max_depth=int(config[\"max_depth\"]),\n",
    "        min_child_weight=int(config[\"min_child_weight\"]),\n",
    "        subsample=config[\"subsample\"],\n",
    "        colsample_bytree=config[\"colsample_bytree\"],\n",
    "        objective='reg:squarederror',\n",
    "        seed=42,\n",
    "        n_jobs=1  # Limit to 1 CPU per trial to prevent over-subscription\n",
    "    )\n",
    "    \n",
    "    # Train the model\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Predict on the validation set\n",
    "    y_pred = model.predict(X_val)\n",
    "    \n",
    "    # Calculate R² on the validation set\n",
    "    r2 = r2_score(y_val, y_pred)\n",
    "    \n",
    "    # Report the metric to Ray Tune\n",
    "    train.report({\"r2\":r2})\n",
    "\n",
    "# Start an MLflow run\n",
    "with mlflow.start_run():\n",
    "    # Log parameters\n",
    "    path = \"Synthetic_data/experiment_results.csv\"\n",
    "    mlflow.log_param(\"data_path\", path)\n",
    "    \n",
    "    # Load data\n",
    "    df = pd.read_csv(path)\n",
    "    mlflow.log_param(\"data_shape\", df.shape)\n",
    "    \n",
    "    # Define the columns of interest (features)\n",
    "    columns_of_interest = [\n",
    "        'charge_c_rate_modulation', 'protocol_choice_prob', 'charge_soc_modulation',\n",
    "        'rest_duration_modulation', 'discharge_factor_modulation', 'discharge_soc_modulation'\n",
    "    ]\n",
    "    \n",
    "    # Log the features used\n",
    "    mlflow.log_param(\"columns_of_interest\", columns_of_interest)\n",
    "    \n",
    "    # Prepare the data (this will be passed to the training function)\n",
    "    data = df.copy()\n",
    "    \n",
    "    # Initialize HyperOptSearch\n",
    "    hyperopt_search = HyperOptSearch(\n",
    "        space=search_space,\n",
    "        metric=\"r2\",\n",
    "        mode=\"max\"\n",
    "    )\n",
    "    \n",
    "    # Initialize HyperBandScheduler for iterative, race-like optimization\n",
    "    scheduler = HyperBandScheduler(\n",
    "        metric=\"r2\",\n",
    "        mode=\"max\",\n",
    "        max_t=10,\n",
    "        reduction_factor=3\n",
    "    )\n",
    "    \n",
    "    # Run hyperparameter tuning\n",
    "    analysis = tune.run(\n",
    "        tune.with_parameters(train_model, data=data, columns_of_interest=columns_of_interest),\n",
    "        search_alg=hyperopt_search,\n",
    "        num_samples=200,  # Number of hyperparameter configurations to try\n",
    "        scheduler=scheduler,\n",
    "        resources_per_trial={\"cpu\": 1},  # Adjust based on your resources\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # Get the best hyperparameters\n",
    "    best_config = analysis.get_best_config(metric=\"r2\", mode=\"max\")\n",
    "    \n",
    "    # Log the best hyperparameters\n",
    "    mlflow.log_params(best_config)\n",
    "    \n",
    "    # Extract the best threshold\n",
    "    best_threshold = best_config[\"threshold\"]\n",
    "    \n",
    "    # Recompute the target variable 'y' using the best threshold\n",
    "    iteration_columns = data.filter(regex='^Iteration').copy()\n",
    "    iteration_columns = iteration_columns.apply(pd.to_numeric, errors='coerce')\n",
    "    iteration_columns = iteration_columns.replace(0, np.nan)\n",
    "    soh = iteration_columns.div(iteration_columns.iloc[:, 0], axis=0)\n",
    "    \n",
    "    soh_threshold = soh.apply(\n",
    "        lambda row: next((i for i, v in enumerate(row) if v <= best_threshold), len(row)),\n",
    "        axis=1\n",
    "    )\n",
    "    soh_threshold = soh_threshold.astype(float)\n",
    "    \n",
    "    # Prepare the final dataset with the best threshold\n",
    "    df_with_threshold = data[columns_of_interest].copy()\n",
    "    df_with_threshold['target'] = soh_threshold\n",
    "    df_with_threshold = df_with_threshold.dropna()\n",
    "    \n",
    "    # Split features and target\n",
    "    X = df_with_threshold[columns_of_interest]\n",
    "    y = df_with_threshold['target']\n",
    "    \n",
    "    # Train the final model on the entire dataset with best hyperparameters\n",
    "    best_model = xgb.XGBRegressor(\n",
    "        learning_rate=best_config[\"learning_rate\"],\n",
    "        max_depth=int(best_config[\"max_depth\"]),\n",
    "        min_child_weight=int(best_config[\"min_child_weight\"]),\n",
    "        subsample=best_config[\"subsample\"],\n",
    "        colsample_bytree=best_config[\"colsample_bytree\"],\n",
    "        objective='reg:squarederror',\n",
    "        seed=42\n",
    "    )\n",
    "    \n",
    "    best_model.fit(X, y)\n",
    "    \n",
    "    # Calculate SHAP values\n",
    "    explainer = shap.Explainer(best_model)\n",
    "    shap_values = explainer(X)\n",
    "    \n",
    "    # Determine the most important feature\n",
    "    shap_importance = np.abs(shap_values.values).mean(axis=0)\n",
    "    most_important_feature = columns_of_interest[np.argmax(shap_importance)]\n",
    "    \n",
    "    # Log the most important feature\n",
    "    mlflow.log_param(\"most_important_feature\", most_important_feature)\n",
    "    \n",
    "    # Predict the values\n",
    "    y_pred = best_model.predict(X)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    mae = mean_absolute_error(y, y_pred)\n",
    "    mse = mean_squared_error(y, y_pred)\n",
    "    r2 = r2_score(y, y_pred)\n",
    "    \n",
    "    # Log metrics\n",
    "    mlflow.log_metric(\"MAE\", mae)\n",
    "    mlflow.log_metric(\"MSE\", mse)\n",
    "    mlflow.log_metric(\"R2\", r2)\n",
    "    \n",
    "    # Log the model with input example\n",
    "    input_example = X.iloc[:5]\n",
    "    mlflow.xgboost.log_model(best_model, \"best_model\", input_example=input_example)\n",
    "    \n",
    "    # Print the most important feature and metrics\n",
    "    print(f\"The most important feature is {most_important_feature}\")\n",
    "    print(f\"Metrics: MAE = {mae:.4f}, MSE = {mse:.4f}, R² = {r2:.4f}\")\n",
    "    \n",
    "    # Plot the SHAP values\n",
    "    shap.summary_plot(shap_values, X, feature_names=columns_of_interest, show=False)\n",
    "    plt.savefig(\"shap_summary.png\", bbox_inches='tight')\n",
    "    mlflow.log_artifact(\"shap_summary.png\")\n",
    "    plt.close()\n",
    "    \n",
    "    shap.dependence_plot(\n",
    "        most_important_feature, shap_values.values, X,\n",
    "        feature_names=columns_of_interest, show=False\n",
    "    )\n",
    "    plt.savefig(\"shap_dependence.png\", bbox_inches='tight')\n",
    "    mlflow.log_artifact(\"shap_dependence.png\")\n",
    "    plt.close()\n",
    "    \n",
    "    # Save the SHAP values\n",
    "    shap_values_file = \"shap_values.pkl\"\n",
    "    with open(shap_values_file, \"wb\") as f:\n",
    "        pickle.dump(shap_values, f)\n",
    "    mlflow.log_artifact(shap_values_file)\n",
    "    \n",
    "    # Log the experimental setup\n",
    "    mlflow.log_param(\"experimental_setup\", {\n",
    "        \"data_path\": path,\n",
    "        \"columns_of_interest\": columns_of_interest\n",
    "    })\n",
    "    \n",
    "    # Log the preprocessing steps\n",
    "    mlflow.log_param(\"preprocessing_steps\", {\n",
    "        \"iteration_columns_conversion\": \"Converted to numeric and replaced zeros with NaN\",\n",
    "        \"soh_calculation\": \"Calculated SoH and created thresholds\"\n",
    "    })\n",
    "    \n",
    "    # Log the model training and evaluation details\n",
    "    mlflow.log_param(\"model_training\", {\n",
    "        \"model_type\": \"XGBoost\",\n",
    "        \"evaluation_metrics\": [\"MAE\", \"MSE\", \"R²\"],\n",
    "        \"hyperparameter_tuning\": \"Used Ray Tune with HyperOptSearch and HyperBandScheduler, maximizing R²\"\n",
    "    })\n",
    "    \n",
    "    # Track the specific file of the experimental result\n",
    "    mlflow.log_artifact(path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import hyperopt\n",
    "import mlflow\n",
    "import mlflow.xgboost\n",
    "import shap\n",
    "import xgboost as xgb\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from ray import tune, train\n",
    "from ray.tune.schedulers import HyperBandScheduler\n",
    "from ray.tune.search.hyperopt import HyperOptSearch\n",
    "\n",
    "# Suppress XGBoost model format warning\n",
    "warnings.filterwarnings(action='ignore', category=UserWarning, module='xgboost')\n",
    "\n",
    "# Initialize MLflow experiment\n",
    "mlflow.set_experiment(\"New SoH Prediction Experiment\")\n",
    "\n",
    "# Define the search space for hyperparameters, including 'threshold'\n",
    "search_space = {\n",
    "    \"learning_rate\": hyperopt.hp.loguniform(\"learning_rate\", np.log(0.01), np.log(0.1)),\n",
    "    \"max_depth\": hyperopt.hp.randint(\"max_depth\", 3, 10),\n",
    "    \"min_child_weight\": hyperopt.hp.randint(\"min_child_weight\", 1, 6),\n",
    "    \"subsample\": hyperopt.hp.uniform(\"subsample\", 0.5, 1.0),\n",
    "    \"colsample_bytree\": hyperopt.hp.uniform(\"colsample_bytree\", 0.5, 1.0),\n",
    "    \"threshold\": hyperopt.hp.choice(\"threshold\", [0.98, 0.95, 0.9, 0.85, 0.8])  # Threshold as a hyperparameter\n",
    "}\n",
    "\n",
    "# Start an MLflow run\n",
    "with mlflow.start_run():\n",
    "    # Log parameters\n",
    "    path = \"Synthetic_data/experiment_results.csv\"\n",
    "    mlflow.log_param(\"data_path\", path)\n",
    "    \n",
    "    # Load data\n",
    "    df = pd.read_csv(path)\n",
    "    mlflow.log_param(\"data_shape\", df.shape)\n",
    "    \n",
    "    # Define the columns of interest (features)\n",
    "    columns_of_interest = [\n",
    "        'charge_c_rate_modulation', 'protocol_choice_prob', 'charge_soc_modulation',\n",
    "        'rest_duration_modulation', 'discharge_factor_modulation', 'discharge_soc_modulation'\n",
    "    ]\n",
    "    \n",
    "    # Log the features used\n",
    "    mlflow.log_param(\"columns_of_interest\", columns_of_interest)\n",
    "    \n",
    "    # Prepare the data\n",
    "    data = df.copy()\n",
    "    \n",
    "    # Split the data into training+validation (90%) and test (10%) sets\n",
    "    train_val_data, test_data = train_test_split(\n",
    "        data, test_size=0.1, random_state=42\n",
    "    )\n",
    "    \n",
    "    # Log the sizes of the datasets\n",
    "    mlflow.log_param(\"train_val_data_shape\", train_val_data.shape)\n",
    "    mlflow.log_param(\"test_data_shape\", test_data.shape)\n",
    "    \n",
    "    # Define the training function\n",
    "    def train_model(config, train_val_data, columns_of_interest):\n",
    "        import xgboost as xgb\n",
    "        from sklearn.metrics import r2_score\n",
    "        from sklearn.model_selection import train_test_split\n",
    "        import pandas as pd\n",
    "        import numpy as np\n",
    "        \n",
    "        # Extract the threshold from the config\n",
    "        threshold = config[\"threshold\"]\n",
    "        \n",
    "        # Preprocess data based on the threshold\n",
    "        # Calculate SoH (State of Health) based on the iteration columns\n",
    "        iteration_columns = train_val_data.filter(regex='^Iteration').copy()\n",
    "        iteration_columns = iteration_columns.apply(pd.to_numeric, errors='coerce')\n",
    "        iteration_columns = iteration_columns.replace(0, np.nan)\n",
    "        soh = iteration_columns.div(iteration_columns.iloc[:, 0], axis=0)\n",
    "        \n",
    "        # Calculate the target variable 'y' based on the threshold\n",
    "        soh_threshold = soh.apply(\n",
    "            lambda row: next((i for i, v in enumerate(row) if v <= threshold), len(row)),\n",
    "            axis=1\n",
    "        )\n",
    "        soh_threshold = soh_threshold.astype(float)\n",
    "        \n",
    "        # Combine features and target into a single DataFrame\n",
    "        df_with_threshold = train_val_data[columns_of_interest].copy()\n",
    "        df_with_threshold['target'] = soh_threshold\n",
    "        df_with_threshold = df_with_threshold.dropna()\n",
    "        \n",
    "        # Split features and target\n",
    "        X = df_with_threshold[columns_of_interest]\n",
    "        y = df_with_threshold['target']\n",
    "        \n",
    "        # Split data into training and validation sets (within train_val_data)\n",
    "        X_train, X_val, y_train, y_val = train_test_split(\n",
    "            X, y, test_size=0.2, random_state=42\n",
    "        )\n",
    "        \n",
    "        # Initialize the XGBoost regressor with hyperparameters from 'config'\n",
    "        model = xgb.XGBRegressor(\n",
    "            learning_rate=config[\"learning_rate\"],\n",
    "            max_depth=int(config[\"max_depth\"]),\n",
    "            min_child_weight=int(config[\"min_child_weight\"]),\n",
    "            subsample=config[\"subsample\"],\n",
    "            colsample_bytree=config[\"colsample_bytree\"],\n",
    "            objective='reg:squarederror',\n",
    "            seed=42,\n",
    "            n_jobs=1  # Limit to 1 CPU per trial to prevent over-subscription\n",
    "        )\n",
    "        \n",
    "        # Train the model\n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        # Predict on the validation set\n",
    "        y_pred = model.predict(X_val)\n",
    "        \n",
    "        # Calculate R² on the validation set\n",
    "        r2 = r2_score(y_val, y_pred)\n",
    "        \n",
    "        # Report the metric to Ray Tune\n",
    "        train.report({\"r2\":r2})\n",
    "    \n",
    "    # Initialize HyperOptSearch\n",
    "    hyperopt_search = HyperOptSearch(\n",
    "        space=search_space,\n",
    "        metric=\"r2\",\n",
    "        mode=\"max\"\n",
    "    )\n",
    "    \n",
    "    # Initialize HyperBandScheduler for iterative, race-like optimization\n",
    "    scheduler = HyperBandScheduler(\n",
    "        metric=\"r2\",\n",
    "        mode=\"max\",\n",
    "        max_t=10,\n",
    "        reduction_factor=3\n",
    "    )\n",
    "    \n",
    "    # Run hyperparameter tuning\n",
    "    analysis = tune.run(\n",
    "        tune.with_parameters(train_model, train_val_data=train_val_data, columns_of_interest=columns_of_interest),\n",
    "        search_alg=hyperopt_search,\n",
    "        num_samples=200,  # Number of hyperparameter configurations to try\n",
    "        scheduler=scheduler,\n",
    "        resources_per_trial={\"cpu\": 1},  # Adjust based on your resources\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # Get the best hyperparameters\n",
    "    best_config = analysis.get_best_config(metric=\"r2\", mode=\"max\")\n",
    "    \n",
    "    # Log the best hyperparameters\n",
    "    mlflow.log_params(best_config)\n",
    "    \n",
    "    # Extract the best threshold\n",
    "    best_threshold = best_config[\"threshold\"]\n",
    "    \n",
    "    # Recompute the target variable 'y' using the best threshold on the combined train_val_data\n",
    "    iteration_columns = train_val_data.filter(regex='^Iteration').copy()\n",
    "    iteration_columns = iteration_columns.apply(pd.to_numeric, errors='coerce')\n",
    "    iteration_columns = iteration_columns.replace(0, np.nan)\n",
    "    soh = iteration_columns.div(iteration_columns.iloc[:, 0], axis=0)\n",
    "    \n",
    "    soh_threshold = soh.apply(\n",
    "        lambda row: next((i for i, v in enumerate(row) if v <= best_threshold), len(row)),\n",
    "        axis=1\n",
    "    )\n",
    "    soh_threshold = soh_threshold.astype(float)\n",
    "    \n",
    "    # Prepare the final dataset with the best threshold\n",
    "    df_with_threshold = train_val_data[columns_of_interest].copy()\n",
    "    df_with_threshold['target'] = soh_threshold\n",
    "    df_with_threshold = df_with_threshold.dropna()\n",
    "    \n",
    "    # Split features and target\n",
    "    X_train_val = df_with_threshold[columns_of_interest]\n",
    "    y_train_val = df_with_threshold['target']\n",
    "    \n",
    "    # Retrain the final model on the combined training and validation data\n",
    "    best_model = xgb.XGBRegressor(\n",
    "        learning_rate=best_config[\"learning_rate\"],\n",
    "        max_depth=int(best_config[\"max_depth\"]),\n",
    "        min_child_weight=int(best_config[\"min_child_weight\"]),\n",
    "        subsample=best_config[\"subsample\"],\n",
    "        colsample_bytree=best_config[\"colsample_bytree\"],\n",
    "        objective='reg:squarederror',\n",
    "        seed=42\n",
    "    )\n",
    "    \n",
    "    best_model.fit(X_train_val, y_train_val)\n",
    "    \n",
    "    # Prepare the test set for evaluation\n",
    "    # Preprocess the test data using the best threshold\n",
    "    iteration_columns_test = test_data.filter(regex='^Iteration').copy()\n",
    "    iteration_columns_test = iteration_columns_test.apply(pd.to_numeric, errors='coerce')\n",
    "    iteration_columns_test = iteration_columns_test.replace(0, np.nan)\n",
    "    soh_test = iteration_columns_test.div(iteration_columns_test.iloc[:, 0], axis=0)\n",
    "    \n",
    "    soh_threshold_test = soh_test.apply(\n",
    "        lambda row: next((i for i, v in enumerate(row) if v <= best_threshold), len(row)),\n",
    "        axis=1\n",
    "    )\n",
    "    soh_threshold_test = soh_threshold_test.astype(float)\n",
    "    \n",
    "    # Prepare the test dataset with the best threshold\n",
    "    df_test_with_threshold = test_data[columns_of_interest].copy()\n",
    "    df_test_with_threshold['target'] = soh_threshold_test\n",
    "    df_test_with_threshold = df_test_with_threshold.dropna()\n",
    "    \n",
    "    # Split features and target for the test set\n",
    "    X_test = df_test_with_threshold[columns_of_interest]\n",
    "    y_test = df_test_with_threshold['target']\n",
    "    \n",
    "    # Evaluate the model on the test set\n",
    "    y_pred_test = best_model.predict(X_test)\n",
    "    \n",
    "    # Calculate metrics on the test set\n",
    "    mae_test = mean_absolute_error(y_test, y_pred_test)\n",
    "    mse_test = mean_squared_error(y_test, y_pred_test)\n",
    "    r2_test = r2_score(y_test, y_pred_test)\n",
    "    \n",
    "    # Log test metrics\n",
    "    mlflow.log_metric(\"Test_MAE\", mae_test)\n",
    "    mlflow.log_metric(\"Test_MSE\", mse_test)\n",
    "    mlflow.log_metric(\"Test_R2\", r2_test)\n",
    "    \n",
    "    # Print the test metrics\n",
    "    print(f\"Test Metrics: MAE = {mae_test:.4f}, MSE = {mse_test:.4f}, R² = {r2_test:.4f}\")\n",
    "    \n",
    "    # Calculate SHAP values on the combined training and validation data\n",
    "    explainer = shap.Explainer(best_model)\n",
    "    shap_values = explainer(X_train_val)\n",
    "    \n",
    "    # Determine the most important feature\n",
    "    shap_importance = np.abs(shap_values.values).mean(axis=0)\n",
    "    most_important_feature = columns_of_interest[np.argmax(shap_importance)]\n",
    "    \n",
    "    # Log the most important feature\n",
    "    mlflow.log_param(\"most_important_feature\", most_important_feature)\n",
    "    \n",
    "    # Predict the values on the combined training and validation data\n",
    "    y_pred_train_val = best_model.predict(X_train_val)\n",
    "    \n",
    "    # Calculate metrics on the combined training and validation data\n",
    "    mae_train_val = mean_absolute_error(y_train_val, y_pred_train_val)\n",
    "    mse_train_val = mean_squared_error(y_train_val, y_pred_train_val)\n",
    "    r2_train_val = r2_score(y_train_val, y_pred_train_val)\n",
    "    \n",
    "    # Log training metrics\n",
    "    mlflow.log_metric(\"Train_Val_MAE\", mae_train_val)\n",
    "    mlflow.log_metric(\"Train_Val_MSE\", mse_train_val)\n",
    "    mlflow.log_metric(\"Train_Val_R2\", r2_train_val)\n",
    "    \n",
    "    # Log the model with input example\n",
    "    input_example = X_train_val.iloc[:5]\n",
    "    mlflow.xgboost.log_model(best_model, \"best_model\", input_example=input_example)\n",
    "    \n",
    "    # Print the most important feature and training metrics\n",
    "    print(f\"The most important feature is {most_important_feature}\")\n",
    "    print(f\"Training Metrics: MAE = {mae_train_val:.4f}, MSE = {mse_train_val:.4f}, R² = {r2_train_val:.4f}\")\n",
    "    \n",
    "    # Plot the SHAP values\n",
    "    shap.summary_plot(shap_values, X_train_val, feature_names=columns_of_interest, show=False)\n",
    "    plt.savefig(\"shap_summary.png\", bbox_inches='tight')\n",
    "    mlflow.log_artifact(\"shap_summary.png\")\n",
    "    plt.close()\n",
    "    \n",
    "    shap.dependence_plot(\n",
    "        most_important_feature, shap_values.values, X_train_val,\n",
    "        feature_names=columns_of_interest, show=False\n",
    "    )\n",
    "    plt.savefig(\"shap_dependence.png\", bbox_inches='tight')\n",
    "    mlflow.log_artifact(\"shap_dependence.png\")\n",
    "    plt.close()\n",
    "    \n",
    "    # Save the SHAP values\n",
    "    shap_values_file = \"shap_values.pkl\"\n",
    "    with open(shap_values_file, \"wb\") as f:\n",
    "        pickle.dump(shap_values, f)\n",
    "    mlflow.log_artifact(shap_values_file)\n",
    "    \n",
    "    # Log the experimental setup\n",
    "    mlflow.log_param(\"experimental_setup\", {\n",
    "        \"data_path\": path,\n",
    "        \"columns_of_interest\": columns_of_interest,\n",
    "        \"test_size\": 0.1  # Indicate that 10% of data was used for testing\n",
    "    })\n",
    "    \n",
    "    # Log the preprocessing steps\n",
    "    mlflow.log_param(\"preprocessing_steps\", {\n",
    "        \"iteration_columns_conversion\": \"Converted to numeric and replaced zeros with NaN\",\n",
    "        \"soh_calculation\": \"Calculated SoH and created thresholds\",\n",
    "        \"data_splitting\": \"Split data into training+validation and test sets\"\n",
    "    })\n",
    "    \n",
    "    # Log the model training and evaluation details\n",
    "    mlflow.log_param(\"model_training\", {\n",
    "        \"model_type\": \"XGBoost\",\n",
    "        \"evaluation_metrics\": [\"MAE\", \"MSE\", \"R²\"],\n",
    "        \"hyperparameter_tuning\": \"Used Ray Tune with HyperOptSearch and HyperBandScheduler, maximizing R²\"\n",
    "    })\n",
    "    \n",
    "    # Track the specific file of the experimental result\n",
    "    mlflow.log_artifact(path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install ray[tune]\n",
    "\n",
    "import hyperopt\n",
    "import mlflow\n",
    "import mlflow.xgboost\n",
    "import shap\n",
    "import xgboost as xgb\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from ray import tune, train\n",
    "from ray.tune.schedulers import HyperBandScheduler\n",
    "from ray.tune.search.hyperopt import HyperOptSearch\n",
    "from ray.air.integrations.mlflow import MLflowLoggerCallback\n",
    "\n",
    "# Suppress XGBoost model format warning\n",
    "warnings.filterwarnings(action='ignore', category=UserWarning, module='xgboost')\n",
    "\n",
    "# Initialize MLflow experiment\n",
    "mlflow.set_experiment(\"New SoH Prediction Experiment\")\n",
    "\n",
    "# Define the search space for hyperparameters, including 'threshold'\n",
    "search_space = {\n",
    "    \"learning_rate\": hyperopt.hp.loguniform(\"learning_rate\", np.log(0.01), np.log(0.1)),\n",
    "    \"max_depth\": hyperopt.hp.randint(\"max_depth\", 3, 10),\n",
    "    \"min_child_weight\": hyperopt.hp.randint(\"min_child_weight\", 1, 6),\n",
    "    \"subsample\": hyperopt.hp.uniform(\"subsample\", 0.5, 1.0),\n",
    "    \"colsample_bytree\": hyperopt.hp.uniform(\"colsample_bytree\", 0.5, 1.0),\n",
    "    \"threshold\": hyperopt.hp.choice(\"threshold\", [0.98, 0.95, 0.9, 0.85, 0.8])  # Threshold as a hyperparameter\n",
    "}\n",
    "\n",
    "# Start an MLflow run\n",
    "with mlflow.start_run() as parent_run:\n",
    "    # Log parameters\n",
    "    path = \"Synthetic_data/experiment_results.csv\"\n",
    "    mlflow.log_param(\"data_path\", path)\n",
    "\n",
    "    # Load data\n",
    "    df = pd.read_csv(path)\n",
    "    mlflow.log_param(\"data_shape\", df.shape)\n",
    "\n",
    "    # Define the columns of interest (features)\n",
    "    columns_of_interest = [\n",
    "        'charge_c_rate_modulation', 'protocol_choice_prob', 'charge_soc_modulation',\n",
    "        'rest_duration_modulation', 'discharge_factor_modulation', 'discharge_soc_modulation'\n",
    "    ]\n",
    "\n",
    "    # Log the features used\n",
    "    mlflow.log_param(\"columns_of_interest\", columns_of_interest)\n",
    "\n",
    "    # Prepare the data\n",
    "    data = df.copy()\n",
    "\n",
    "    # Split the data into training+validation (90%) and test (10%) sets\n",
    "    train_val_data, test_data = train_test_split(\n",
    "        data, test_size=0.1, random_state=42\n",
    "    )\n",
    "\n",
    "    # Log the sizes of the datasets\n",
    "    mlflow.log_param(\"train_val_data_shape\", train_val_data.shape)\n",
    "    mlflow.log_param(\"test_data_shape\", test_data.shape)\n",
    "\n",
    "    # Define the training function\n",
    "    def train_model(config, train_val_data, columns_of_interest):\n",
    "        import xgboost as xgb\n",
    "        from sklearn.metrics import r2_score\n",
    "        from sklearn.model_selection import train_test_split\n",
    "        import pandas as pd\n",
    "        import numpy as np\n",
    "\n",
    "        # Extract the threshold from the config\n",
    "        threshold = config[\"threshold\"]\n",
    "\n",
    "        # Preprocess data based on the threshold\n",
    "        # Calculate SoH (State of Health) based on the iteration columns\n",
    "        iteration_columns = train_val_data.filter(regex='^Iteration').copy()\n",
    "        iteration_columns = iteration_columns.apply(pd.to_numeric, errors='coerce')\n",
    "        iteration_columns = iteration_columns.replace(0, np.nan)\n",
    "        soh = iteration_columns.div(iteration_columns.iloc[:, 0], axis=0)\n",
    "\n",
    "        # Calculate the target variable 'y' based on the threshold\n",
    "        soh_threshold = soh.apply(\n",
    "            lambda row: next((i for i, v in enumerate(row) if v <= threshold), len(row)),\n",
    "            axis=1\n",
    "        )\n",
    "        soh_threshold = soh_threshold.astype(float)\n",
    "\n",
    "        # Combine features and target into a single DataFrame\n",
    "        df_with_threshold = train_val_data[columns_of_interest].copy()\n",
    "        df_with_threshold['target'] = soh_threshold\n",
    "        df_with_threshold = df_with_threshold.dropna()\n",
    "\n",
    "        # Split features and target\n",
    "        X = df_with_threshold[columns_of_interest]\n",
    "        y = df_with_threshold['target']\n",
    "\n",
    "        # Split data into training and validation sets (within train_val_data)\n",
    "        X_train, X_val, y_train, y_val = train_test_split(\n",
    "            X, y, test_size=0.2, random_state=42\n",
    "        )\n",
    "\n",
    "        # Initialize the XGBoost regressor with hyperparameters from 'config'\n",
    "        model = xgb.XGBRegressor(\n",
    "            learning_rate=config[\"learning_rate\"],\n",
    "            max_depth=int(config[\"max_depth\"]),\n",
    "            min_child_weight=int(config[\"min_child_weight\"]),\n",
    "            subsample=config[\"subsample\"],\n",
    "            colsample_bytree=config[\"colsample_bytree\"],\n",
    "            objective='reg:squarederror',\n",
    "            seed=42,\n",
    "            n_jobs=1  # Limit to 1 CPU per trial to prevent over-subscription\n",
    "        )\n",
    "\n",
    "        # Train the model\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        # Predict on the validation set\n",
    "        y_pred = model.predict(X_val)\n",
    "\n",
    "        # Calculate R² on the validation set\n",
    "        r2 = r2_score(y_val, y_pred)\n",
    "\n",
    "        # Report the metric to Ray Tune\n",
    "        train.report({\"r2\":r2})\n",
    "\n",
    "    # Initialize HyperOptSearch\n",
    "    hyperopt_search = HyperOptSearch(\n",
    "        space=search_space,\n",
    "        metric=\"r2\",\n",
    "        mode=\"max\"\n",
    "    )\n",
    "\n",
    "    # Initialize HyperBandScheduler for iterative, race-like optimization\n",
    "    scheduler = HyperBandScheduler(\n",
    "        metric=\"r2\",\n",
    "        mode=\"max\",\n",
    "        max_t=10,\n",
    "        reduction_factor=3\n",
    "    )\n",
    "\n",
    "    # Set up MLflowLoggerCallback\n",
    "    mlflow_logger = MLflowLoggerCallback(\n",
    "        tracking_uri=mlflow.get_tracking_uri(),\n",
    "        experiment_name=mlflow.get_experiment(mlflow.active_run().info.experiment_id).name,\n",
    "        save_artifact=True\n",
    "    )\n",
    "\n",
    "    # Run hyperparameter tuning\n",
    "    analysis = tune.run(\n",
    "        tune.with_parameters(train_model, train_val_data=train_val_data, columns_of_interest=columns_of_interest),\n",
    "        search_alg=hyperopt_search,\n",
    "        num_samples=50,  # Number of hyperparameter configurations to try\n",
    "        scheduler=scheduler,\n",
    "        resources_per_trial={\"cpu\": 1},  # Adjust based on your resources\n",
    "        verbose=1,\n",
    "        callbacks=[mlflow_logger],\n",
    "        name=\"Ray_Tune_Hyperparameter_Search\"\n",
    "    )\n",
    "\n",
    "    # Get the best hyperparameters\n",
    "    best_config = analysis.get_best_config(metric=\"r2\", mode=\"max\")\n",
    "\n",
    "    # Log the best hyperparameters\n",
    "    mlflow.log_params(best_config)\n",
    "\n",
    "    # Extract the best threshold\n",
    "    best_threshold = best_config[\"threshold\"]\n",
    "\n",
    "    # Recompute the target variable 'y' using the best threshold on the combined train_val_data\n",
    "    iteration_columns = train_val_data.filter(regex='^Iteration').copy()\n",
    "    iteration_columns = iteration_columns.apply(pd.to_numeric, errors='coerce')\n",
    "    iteration_columns = iteration_columns.replace(0, np.nan)\n",
    "    soh = iteration_columns.div(iteration_columns.iloc[:, 0], axis=0)\n",
    "\n",
    "    soh_threshold = soh.apply(\n",
    "        lambda row: next((i for i, v in enumerate(row) if v <= best_threshold), len(row)),\n",
    "        axis=1\n",
    "    )\n",
    "    soh_threshold = soh_threshold.astype(float)\n",
    "\n",
    "    # Prepare the final dataset with the best threshold\n",
    "    df_with_threshold = train_val_data[columns_of_interest].copy()\n",
    "    df_with_threshold['target'] = soh_threshold\n",
    "    df_with_threshold = df_with_threshold.dropna()\n",
    "\n",
    "    # Split features and target\n",
    "    X_train_val = df_with_threshold[columns_of_interest]\n",
    "    y_train_val = df_with_threshold['target']\n",
    "\n",
    "    # Retrain the final model on the combined training and validation data\n",
    "    best_model = xgb.XGBRegressor(\n",
    "        learning_rate=best_config[\"learning_rate\"],\n",
    "        max_depth=int(best_config[\"max_depth\"]),\n",
    "        min_child_weight=int(best_config[\"min_child_weight\"]),\n",
    "        subsample=best_config[\"subsample\"],\n",
    "        colsample_bytree=best_config[\"colsample_bytree\"],\n",
    "        objective='reg:squarederror',\n",
    "        seed=42\n",
    "    )\n",
    "\n",
    "    best_model.fit(X_train_val, y_train_val)\n",
    "\n",
    "    # Prepare the test set for evaluation\n",
    "    # Preprocess the test data using the best threshold\n",
    "    iteration_columns_test = test_data.filter(regex='^Iteration').copy()\n",
    "    iteration_columns_test = iteration_columns_test.apply(pd.to_numeric, errors='coerce')\n",
    "    iteration_columns_test = iteration_columns_test.replace(0, np.nan)\n",
    "    soh_test = iteration_columns_test.div(iteration_columns_test.iloc[:, 0], axis=0)\n",
    "\n",
    "    soh_threshold_test = soh_test.apply(\n",
    "        lambda row: next((i for i, v in enumerate(row) if v <= best_threshold), len(row)),\n",
    "        axis=1\n",
    "    )\n",
    "    soh_threshold_test = soh_threshold_test.astype(float)\n",
    "\n",
    "    # Prepare the test dataset with the best threshold\n",
    "    df_test_with_threshold = test_data[columns_of_interest].copy()\n",
    "    df_test_with_threshold['target'] = soh_threshold_test\n",
    "    df_test_with_threshold = df_test_with_threshold.dropna()\n",
    "\n",
    "    # Split features and target for the test set\n",
    "    X_test = df_test_with_threshold[columns_of_interest]\n",
    "    y_test = df_test_with_threshold['target']\n",
    "\n",
    "    # Evaluate the model on the test set\n",
    "    y_pred_test = best_model.predict(X_test)\n",
    "\n",
    "    # Calculate metrics on the test set\n",
    "    mae_test = mean_absolute_error(y_test, y_pred_test)\n",
    "    mse_test = mean_squared_error(y_test, y_pred_test)\n",
    "    rmse_test = np.sqrt(mse_test)  # Calculate RMSE\n",
    "    r2_test = r2_score(y_test, y_pred_test)\n",
    "\n",
    "    # Log test metrics\n",
    "    mlflow.log_metric(\"Test_MAE\", mae_test)\n",
    "    mlflow.log_metric(\"Test_RMSE\", rmse_test)\n",
    "    mlflow.log_metric(\"Test_R2\", r2_test)\n",
    "\n",
    "    # Print the test metrics\n",
    "    print(f\"Test Metrics: MAE = {mae_test:.4f}, RMSE = {rmse_test:.4f}, R² = {r2_test:.4f}\")\n",
    "\n",
    "    # Calculate SHAP values on the combined training and validation data\n",
    "    explainer = shap.Explainer(best_model)\n",
    "    shap_values = explainer(X_train_val)\n",
    "\n",
    "    # Determine the most important feature\n",
    "    shap_importance = np.abs(shap_values.values).mean(axis=0)\n",
    "    most_important_feature = columns_of_interest[np.argmax(shap_importance)]\n",
    "\n",
    "    # Log the most important feature\n",
    "    mlflow.log_param(\"most_important_feature\", most_important_feature)\n",
    "\n",
    "    # Predict the values on the combined training and validation data\n",
    "    y_pred_train_val = best_model.predict(X_train_val)\n",
    "\n",
    "    # Calculate metrics on the combined training and validation data\n",
    "    mae_train_val = mean_absolute_error(y_train_val, y_pred_train_val)\n",
    "    mse_train_val = mean_squared_error(y_train_val, y_pred_train_val)\n",
    "    rmse_train_val = np.sqrt(mse_train_val)  # Calculate RMSE\n",
    "    r2_train_val = r2_score(y_train_val, y_pred_train_val)\n",
    "\n",
    "    # Log training metrics\n",
    "    mlflow.log_metric(\"Train_Val_MAE\", mae_train_val)\n",
    "    mlflow.log_metric(\"Train_Val_RMSE\", rmse_train_val)\n",
    "    mlflow.log_metric(\"Train_Val_R2\", r2_train_val)\n",
    "\n",
    "    # Log the model with input example\n",
    "    input_example = X_train_val.iloc[:5]\n",
    "    mlflow.xgboost.log_model(best_model, \"best_model\", input_example=input_example)\n",
    "\n",
    "    # Print the most important feature and training metrics\n",
    "    print(f\"The most important feature is {most_important_feature}\")\n",
    "    print(f\"Training Metrics: MAE = {mae_train_val:.4f}, RMSE = {rmse_train_val:.4f}, R² = {r2_train_val:.4f}\")\n",
    "\n",
    "    # Plot the SHAP values\n",
    "    shap.summary_plot(shap_values, X_train_val, feature_names=columns_of_interest, show=False)\n",
    "    plt.savefig(\"shap_summary.png\", bbox_inches='tight')\n",
    "    mlflow.log_artifact(\"shap_summary.png\")\n",
    "    plt.close()\n",
    "\n",
    "    shap.dependence_plot(\n",
    "        most_important_feature, shap_values.values, X_train_val,\n",
    "        feature_names=columns_of_interest, show=False\n",
    "    )\n",
    "    plt.savefig(\"shap_dependence.png\", bbox_inches='tight')\n",
    "    mlflow.log_artifact(\"shap_dependence.png\")\n",
    "    plt.close()\n",
    "\n",
    "    # Save the SHAP values\n",
    "    shap_values_file = \"shap_values.pkl\"\n",
    "    with open(shap_values_file, \"wb\") as f:\n",
    "        pickle.dump(shap_values, f)\n",
    "    mlflow.log_artifact(shap_values_file)\n",
    "\n",
    "    # Log the experimental setup\n",
    "    mlflow.log_param(\"experimental_setup\", {\n",
    "        \"data_path\": path,\n",
    "        \"columns_of_interest\": columns_of_interest,\n",
    "        \"test_size\": 0.1  # Indicate that 10% of data was used for testing\n",
    "    })\n",
    "\n",
    "    # Log the preprocessing steps\n",
    "    mlflow.log_param(\"preprocessing_steps\", {\n",
    "        \"iteration_columns_conversion\": \"Converted to numeric and replaced zeros with NaN\",\n",
    "        \"soh_calculation\": \"Calculated SoH and created thresholds\",\n",
    "        \"data_splitting\": \"Split data into training+validation and test sets\"\n",
    "    })\n",
    "\n",
    "    # Log the model training and evaluation details\n",
    "    mlflow.log_param(\"model_training\", {\n",
    "        \"model_type\": \"XGBoost\",\n",
    "        \"evaluation_metrics\": [\"MAE\", \"RMSE\", \"R²\"],\n",
    "        \"hyperparameter_tuning\": \"Used Ray Tune with HyperOptSearch and HyperBandScheduler, maximizing R²\"\n",
    "    })\n",
    "\n",
    "    # Track the specific file of the experimental result\n",
    "    mlflow.log_artifact(path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install \"ray[tune]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import mlflow\n",
    "import mlflow.xgboost\n",
    "import xgboost as xgb\n",
    "import optuna\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import shap\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "!pip install optuna\n",
    "# Set up MLflow experiment\n",
    "experiment_name = \"New SoH Prediction Experiment with Optuna\"\n",
    "mlflow.set_experiment(experiment_name)\n",
    "\n",
    "# Load data\n",
    "path = \"Synthetic_data/experiment_results.csv\"\n",
    "df = pd.read_csv(path)\n",
    "\n",
    "# Define the columns of interest (features)\n",
    "columns_of_interest = [\n",
    "    'charge_c_rate_modulation', 'protocol_choice_prob', 'charge_soc_modulation',\n",
    "    'rest_duration_modulation', 'discharge_factor_modulation', 'discharge_soc_modulation', \n",
    "]\n",
    "\n",
    "# Prepare the data\n",
    "data = df.copy()\n",
    "\n",
    "# Split the data into training+validation (90%) and test (10%) sets\n",
    "train_val_data, test_data = train_test_split(\n",
    "    data, test_size=0.1, random_state=42\n",
    ")\n",
    "\n",
    "# Start a parent MLflow run\n",
    "with mlflow.start_run(run_name=\"Optuna_Hyperparameter_Tuning\") as parent_run:\n",
    "    parent_run_id = parent_run.info.run_id\n",
    "\n",
    "    # Define the objective function\n",
    "    def objective(trial):\n",
    "        # Start a nested MLflow run for this trial\n",
    "        with mlflow.start_run(run_name=f\"trial_{trial.number}\", nested=True):\n",
    "            # Suggest hyperparameters\n",
    "            learning_rate = trial.suggest_float('learning_rate', 0.01, 0.1, log=True)\n",
    "            max_depth = trial.suggest_int('max_depth', 3, 10)\n",
    "            min_child_weight = trial.suggest_int('min_child_weight', 1, 6)\n",
    "            subsample = trial.suggest_float('subsample', 0.5, 1.0)\n",
    "            colsample_bytree = trial.suggest_float('colsample_bytree', 0.5, 1.0)\n",
    "            threshold = trial.suggest_categorical('threshold', [0.98, 0.95, 0.9, 0.85, 0.8])\n",
    "\n",
    "            # Log hyperparameters\n",
    "            mlflow.log_params({\n",
    "                'learning_rate': learning_rate,\n",
    "                'max_depth': max_depth,\n",
    "                'min_child_weight': min_child_weight,\n",
    "                'subsample': subsample,\n",
    "                'colsample_bytree': colsample_bytree,\n",
    "                'threshold': threshold\n",
    "            })\n",
    "\n",
    "            # Preprocess data based on the threshold\n",
    "            iteration_columns = train_val_data.filter(regex='^Iteration').copy()\n",
    "            iteration_columns = iteration_columns.apply(pd.to_numeric, errors='coerce')\n",
    "            iteration_columns = iteration_columns.replace(0, np.nan)\n",
    "            soh = iteration_columns.div(iteration_columns.iloc[:, 0], axis=0)\n",
    "\n",
    "            # Calculate the target variable 'y' based on the threshold\n",
    "            soh_threshold = soh.apply(\n",
    "                lambda row: next((i for i, v in enumerate(row) if v <= threshold), len(row)),\n",
    "                axis=1\n",
    "            ).astype(float)\n",
    "\n",
    "            # Prepare the dataset\n",
    "            df_with_threshold = train_val_data[columns_of_interest].copy()\n",
    "            df_with_threshold['target'] = soh_threshold\n",
    "            df_with_threshold = df_with_threshold.dropna()\n",
    "\n",
    "            X = df_with_threshold[columns_of_interest]\n",
    "            y = df_with_threshold['target']\n",
    "\n",
    "            # Split data into training and validation sets\n",
    "            X_train, X_val, y_train, y_val = train_test_split(\n",
    "                X, y, test_size=0.2, random_state=42\n",
    "            )\n",
    "\n",
    "            # Initialize the XGBoost regressor\n",
    "            model = xgb.XGBRegressor(\n",
    "                learning_rate=learning_rate,\n",
    "                max_depth=max_depth,\n",
    "                min_child_weight=min_child_weight,\n",
    "                subsample=subsample,\n",
    "                colsample_bytree=colsample_bytree,\n",
    "                objective='reg:squarederror',\n",
    "                seed=42\n",
    "            )\n",
    "\n",
    "            # Train the model\n",
    "            model.fit(X_train, y_train)\n",
    "\n",
    "            # Predict on the validation set\n",
    "            y_pred = model.predict(X_val)\n",
    "\n",
    "            # Calculate metrics on the validation set\n",
    "            r2 = r2_score(y_val, y_pred)\n",
    "            mae_val = mean_absolute_error(y_val, y_pred)\n",
    "            rmse_val = np.sqrt(mean_squared_error(y_val, y_pred))\n",
    "\n",
    "            # Log metrics\n",
    "            mlflow.log_metrics({\n",
    "                'validation_R2': r2,\n",
    "                'validation_MAE': mae_val,\n",
    "                'validation_RMSE': rmse_val\n",
    "            })\n",
    "\n",
    "            # Return the validation R² to be maximized\n",
    "            return rmse_val\n",
    "\n",
    "    # Remove MLflowCallback since we're handling logging manually\n",
    "    # Create an Optuna study\n",
    "    study = optuna.create_study(\n",
    "        direction='minimize',\n",
    "        study_name='XGBoost_SoH_Optimization',\n",
    "        sampler=optuna.samplers.TPESampler(seed=42)\n",
    "    )\n",
    "\n",
    "    # Optimize the objective function\n",
    "    study.optimize(objective, n_trials=50)\n",
    "\n",
    "    # Get the best hyperparameters\n",
    "    best_params = study.best_params\n",
    "\n",
    "    # Extract the best threshold\n",
    "    best_threshold = best_params.pop('threshold')\n",
    "\n",
    "    # Recompute the target variable 'y' using the best threshold on the combined train_val_data\n",
    "    iteration_columns = train_val_data.filter(regex='^Iteration').copy()\n",
    "    iteration_columns = iteration_columns.apply(pd.to_numeric, errors='coerce')\n",
    "    iteration_columns = iteration_columns.replace(0, np.nan)\n",
    "    soh = iteration_columns.div(iteration_columns.iloc[:, 0], axis=0)\n",
    "\n",
    "    soh_threshold = soh.apply(\n",
    "        lambda row: next((i for i, v in enumerate(row) if v <= best_threshold), len(row)),\n",
    "        axis=1\n",
    "    ).astype(float)\n",
    "\n",
    "    # Prepare the final dataset with the best threshold\n",
    "    df_with_threshold = train_val_data[columns_of_interest].copy()\n",
    "    df_with_threshold['target'] = soh_threshold\n",
    "    df_with_threshold = df_with_threshold.dropna()\n",
    "\n",
    "    X_train_val = df_with_threshold[columns_of_interest]\n",
    "    y_train_val = df_with_threshold['target']\n",
    "\n",
    "    # Initialize the final model\n",
    "    final_model = xgb.XGBRegressor(\n",
    "        objective='reg:squarederror',\n",
    "        seed=42,\n",
    "        **best_params  # Unpack the best hyperparameters\n",
    "    )\n",
    "\n",
    "    # Train the final model\n",
    "    final_model.fit(X_train_val, y_train_val)\n",
    "\n",
    "    # Prepare the test set\n",
    "    iteration_columns_test = test_data.filter(regex='^Iteration').copy()\n",
    "    iteration_columns_test = iteration_columns_test.apply(pd.to_numeric, errors='coerce')\n",
    "    iteration_columns_test = iteration_columns_test.replace(0, np.nan)\n",
    "    soh_test = iteration_columns_test.div(iteration_columns_test.iloc[:, 0], axis=0)\n",
    "\n",
    "    soh_threshold_test = soh_test.apply(\n",
    "        lambda row: next((i for i, v in enumerate(row) if v <= best_threshold), len(row)),\n",
    "        axis=1\n",
    "    ).astype(float)\n",
    "\n",
    "    df_test_with_threshold = test_data[columns_of_interest].copy()\n",
    "    df_test_with_threshold['target'] = soh_threshold_test\n",
    "    df_test_with_threshold = df_test_with_threshold.dropna()\n",
    "\n",
    "    X_test = df_test_with_threshold[columns_of_interest]\n",
    "    y_test = df_test_with_threshold['target']\n",
    "\n",
    "    # Evaluate the model on the test set\n",
    "    y_pred_test = final_model.predict(X_test)\n",
    "\n",
    "    # Calculate metrics on the test set\n",
    "    mae_test = mean_absolute_error(y_test, y_pred_test)\n",
    "    rmse_test = np.sqrt(mean_squared_error(y_test, y_pred_test))\n",
    "    r2_test = r2_score(y_test, y_pred_test)\n",
    "\n",
    "    # Start a nested MLflow run for the final model\n",
    "    with mlflow.start_run(run_name=\"Final_Model\", nested=True):\n",
    "        # Log best hyperparameters\n",
    "        mlflow.log_params(best_params)\n",
    "        mlflow.log_param(\"threshold\", best_threshold)\n",
    "\n",
    "        # Log test metrics\n",
    "        mlflow.log_metrics({\n",
    "            \"Test_MAE\": mae_test,\n",
    "            \"Test_RMSE\": rmse_test,\n",
    "            \"Test_R2\": r2_test\n",
    "        })\n",
    "\n",
    "        # Calculate SHAP values\n",
    "        explainer = shap.Explainer(final_model)\n",
    "        shap_values = explainer(X_train_val)\n",
    "\n",
    "        # Determine the most important feature\n",
    "        shap_importance = np.abs(shap_values.values).mean(axis=0)\n",
    "        most_important_feature = columns_of_interest[np.argmax(shap_importance)]\n",
    "        mlflow.log_param(\"most_important_feature\", most_important_feature)\n",
    "\n",
    "        # Log the model\n",
    "        mlflow.xgboost.log_model(final_model, \"final_model\")\n",
    "\n",
    "        # Plot SHAP summary\n",
    "        shap.summary_plot(shap_values, X_train_val, feature_names=columns_of_interest, show=False)\n",
    "        plt.savefig(\"shap_summary.png\", bbox_inches='tight')\n",
    "        mlflow.log_artifact(\"shap_summary.png\")\n",
    "        plt.close()\n",
    "\n",
    "        # Plot SHAP dependence for the most important feature\n",
    "        shap.dependence_plot(\n",
    "            most_important_feature, shap_values.values, X_train_val,\n",
    "            feature_names=columns_of_interest, show=False\n",
    "        )\n",
    "        plt.savefig(\"shap_dependence.png\", bbox_inches='tight')\n",
    "        mlflow.log_artifact(\"shap_dependence.png\")\n",
    "        plt.close()\n",
    "\n",
    "        # Save SHAP values\n",
    "        shap_values_file = \"shap_values.pkl\"\n",
    "        with open(shap_values_file, \"wb\") as f:\n",
    "            pickle.dump(shap_values, f)\n",
    "        mlflow.log_artifact(shap_values_file)\n",
    "\n",
    "        # Log data information\n",
    "        mlflow.log_param(\"data_path\", path)\n",
    "        mlflow.log_param(\"data_shape\", df.shape)\n",
    "        mlflow.log_param(\"columns_of_interest\", columns_of_interest)\n",
    "        mlflow.log_param(\"train_val_data_shape\", train_val_data.shape)\n",
    "        mlflow.log_param(\"test_data_shape\", test_data.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import mlflow\n",
    "import mlflow.xgboost\n",
    "import xgboost as xgb\n",
    "import optuna\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import shap\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Set up MLflow experiment\n",
    "experiment_name = \"New SoH Prediction Experiment with Optuna\"\n",
    "mlflow.set_experiment(experiment_name)\n",
    "\n",
    "# Load data\n",
    "path = \"Synthetic_data/experiment_results.csv\"\n",
    "df = pd.read_csv(path)\n",
    "\n",
    "#\n",
    "# Define the columns of interest (features)\n",
    "columns_of_interest = [\n",
    "    'charge_c_rate_modulation', 'protocol_choice_prob', 'charge_soc_modulation',\n",
    "    'rest_duration_modulation', 'discharge_factor_modulation', 'discharge_soc_modulation', 'discharge_column_prob'\n",
    "]\n",
    "\n",
    "# Prepare the data\n",
    "data = df.copy()\n",
    "\n",
    "# Split the data into training+validation (90%) and test (10%) sets\n",
    "train_val_data, test_data = train_test_split(\n",
    "    data, test_size=0.1, random_state=42\n",
    ")\n",
    "\n",
    "# Start a parent MLflow run\n",
    "with mlflow.start_run(run_name=\"Optuna_Hyperparameter_Tuning\") as parent_run:\n",
    "    parent_run_id = parent_run.info.run_id\n",
    "\n",
    "    # Define the objective function\n",
    "    def objective(trial):\n",
    "        # Start a nested MLflow run for this trial\n",
    "        with mlflow.start_run(run_name=f\"trial_{trial.number}\", nested=True):\n",
    "            # Suggest hyperparameters\n",
    "            learning_rate = trial.suggest_float('learning_rate', 0.01, 0.1, log=True)\n",
    "            max_depth = trial.suggest_int('max_depth', 3, 10)\n",
    "            min_child_weight = trial.suggest_int('min_child_weight', 1, 6)\n",
    "            subsample = trial.suggest_float('subsample', 0.5, 1.0)\n",
    "            colsample_bytree = trial.suggest_float('colsample_bytree', 0.5, 1.0)\n",
    "            threshold = trial.suggest_categorical('threshold', [0.98, 0.95, 0.9, 0.85, 0.8])\n",
    "\n",
    "            # Log hyperparameters\n",
    "            mlflow.log_params({\n",
    "                'learning_rate': learning_rate,\n",
    "                'max_depth': max_depth,\n",
    "                'min_child_weight': min_child_weight,\n",
    "                'subsample': subsample,\n",
    "                'colsample_bytree': colsample_bytree,\n",
    "                'threshold': threshold\n",
    "            })\n",
    "\n",
    "            # Preprocess data based on the threshold\n",
    "            iteration_columns = train_val_data.filter(regex='^Iteration').copy()\n",
    "            iteration_columns = iteration_columns.apply(pd.to_numeric, errors='coerce')\n",
    "            iteration_columns = iteration_columns.replace(0, np.nan)\n",
    "            soh = iteration_columns.div(iteration_columns.iloc[:, 0], axis=0)\n",
    "\n",
    "            # Calculate the target variable 'y' based on the threshold\n",
    "            soh_threshold = soh.apply(\n",
    "                lambda row: next((i for i, v in enumerate(row) if v <= threshold), len(row)),\n",
    "                axis=1\n",
    "            ).astype(float)\n",
    "\n",
    "            # Prepare the dataset\n",
    "            df_with_threshold = train_val_data[columns_of_interest].copy()\n",
    "            df_with_threshold['target'] = soh_threshold\n",
    "            df_with_threshold = df_with_threshold.dropna()\n",
    "             # **Discretize and convert to categorical dtype**\n",
    "            # Discretize 'protocol_choice_prob' into 20 categories\n",
    "            df_with_threshold['protocol_choice_prob'] = pd.cut(\n",
    "                df_with_threshold['protocol_choice_prob'],\n",
    "                bins=10,\n",
    "                labels=False,\n",
    "                include_lowest=True\n",
    "            ).astype('category')\n",
    "\n",
    "            # Discretize 'discharge_column_prob' into 100 categories\n",
    "            df_with_threshold['discharge_column_prob'] = pd.cut(\n",
    "                df_with_threshold['discharge_column_prob'],\n",
    "                bins=40,\n",
    "                labels=False,\n",
    "                include_lowest=True\n",
    "            ).astype('category')\n",
    "\n",
    "            X = df_with_threshold[columns_of_interest]\n",
    "            y = df_with_threshold['target']\n",
    "\n",
    "            # Split data into training and validation sets\n",
    "            X_train, X_val, y_train, y_val = train_test_split(\n",
    "                X, y, test_size=0.2, random_state=42\n",
    "            )\n",
    "\n",
    "            # Initialize the XGBoost regressor\n",
    "            model = xgb.XGBRegressor(\n",
    "                learning_rate=learning_rate,\n",
    "                max_depth=max_depth,\n",
    "                min_child_weight=min_child_weight,\n",
    "                subsample=subsample,\n",
    "                colsample_bytree=colsample_bytree,\n",
    "                objective='reg:squarederror',\n",
    "                seed=42,\n",
    "                enable_categorical=True\n",
    "            )\n",
    "\n",
    "            # Train the model\n",
    "            model.fit(X_train, y_train)\n",
    "\n",
    "            # Predict on the validation set\n",
    "            y_pred = model.predict(X_val)\n",
    "\n",
    "            # Calculate metrics on the validation set\n",
    "            r2 = r2_score(y_val, y_pred)\n",
    "            mae_val = mean_absolute_error(y_val, y_pred)\n",
    "            rmse_val = np.sqrt(mean_squared_error(y_val, y_pred))\n",
    "\n",
    "            # Log metrics\n",
    "            mlflow.log_metrics({\n",
    "                'validation_R2': r2,\n",
    "                'validation_MAE': mae_val,\n",
    "                'validation_RMSE': rmse_val\n",
    "            })\n",
    "\n",
    "            # Return the validation R² to be maximized\n",
    "            return rmse_val\n",
    "\n",
    "    # Remove MLflowCallback since we're handling logging manually\n",
    "    # Create an Optuna study\n",
    "    study = optuna.create_study(\n",
    "        direction='minimize',\n",
    "        study_name='XGBoost_SoH_Optimization',\n",
    "        sampler=optuna.samplers.TPESampler(seed=42)\n",
    "    )\n",
    "\n",
    "    # Optimize the objective function\n",
    "    study.optimize(objective, n_trials=300)\n",
    "\n",
    "    # Get the best hyperparameters\n",
    "    best_params = study.best_params\n",
    "\n",
    "    # Extract the best threshold\n",
    "    best_threshold = best_params.pop('threshold')\n",
    "\n",
    "    # Recompute the target variable 'y' using the best threshold on the combined train_val_data\n",
    "    iteration_columns = train_val_data.filter(regex='^Iteration').copy()\n",
    "    iteration_columns = iteration_columns.apply(pd.to_numeric, errors='coerce')\n",
    "    iteration_columns = iteration_columns.replace(0, np.nan)\n",
    "    soh = iteration_columns.div(iteration_columns.iloc[:, 0], axis=0)\n",
    "\n",
    "    soh_threshold = soh.apply(\n",
    "        lambda row: next((i for i, v in enumerate(row) if v <= best_threshold), len(row)),\n",
    "        axis=1\n",
    "    ).astype(float)\n",
    "\n",
    "    # Prepare the final dataset with the best threshold\n",
    "    df_with_threshold = train_val_data[columns_of_interest].copy()\n",
    "    df_with_threshold['target'] = soh_threshold\n",
    "    df_with_threshold = df_with_threshold.dropna()\n",
    "    # Discretize and convert to categorical dtype\n",
    "    df_with_threshold['protocol_choice_prob'] = pd.cut(\n",
    "        df_with_threshold['protocol_choice_prob'],\n",
    "        bins=10,\n",
    "        labels=False,\n",
    "        include_lowest=True\n",
    "    ).astype('category')\n",
    "\n",
    "    df_with_threshold['discharge_column_prob'] = pd.cut(\n",
    "        df_with_threshold['discharge_column_prob'],\n",
    "        bins=40,\n",
    "        labels=False,\n",
    "        include_lowest=True\n",
    "    ).astype('category')\n",
    "    X_train_val = df_with_threshold[columns_of_interest]\n",
    "    y_train_val = df_with_threshold['target']\n",
    "\n",
    "    final_model = xgb.XGBRegressor(\n",
    "        objective='reg:squarederror',\n",
    "        seed=42,\n",
    "        enable_categorical=True,\n",
    "        **best_params  # Unpack the best hyperparameters\n",
    "    )\n",
    "    \n",
    "\n",
    "    # Train the final model\n",
    "    final_model.fit(X_train_val, y_train_val)\n",
    "\n",
    "    # Prepare the test set\n",
    "    iteration_columns_test = test_data.filter(regex='^Iteration').copy()\n",
    "    iteration_columns_test = iteration_columns_test.apply(pd.to_numeric, errors='coerce')\n",
    "    iteration_columns_test = iteration_columns_test.replace(0, np.nan)\n",
    "    soh_test = iteration_columns_test.div(iteration_columns_test.iloc[:, 0], axis=0)\n",
    "\n",
    "    soh_threshold_test = soh_test.apply(\n",
    "        lambda row: next((i for i, v in enumerate(row) if v <= best_threshold), len(row)),\n",
    "        axis=1\n",
    "    ).astype(float)\n",
    "\n",
    "    df_test_with_threshold = test_data[columns_of_interest].copy()\n",
    "    df_test_with_threshold['target'] = soh_threshold_test\n",
    "    df_test_with_threshold = df_test_with_threshold.dropna()\n",
    "\n",
    "    X_test = df_test_with_threshold[columns_of_interest]\n",
    "    y_test = df_test_with_threshold['target']\n",
    "\n",
    "    # Evaluate the model on the test set\n",
    "    y_pred_test = final_model.predict(X_test)\n",
    "\n",
    "    # Calculate metrics on the test set\n",
    "    mae_test = mean_absolute_error(y_test, y_pred_test)\n",
    "    rmse_test = np.sqrt(mean_squared_error(y_test, y_pred_test))\n",
    "    r2_test = r2_score(y_test, y_pred_test)\n",
    "\n",
    "    # Start a nested MLflow run for the final model\n",
    "    with mlflow.start_run(run_name=\"Final_Model\", nested=True):\n",
    "        # Log best hyperparameters\n",
    "        mlflow.log_params(best_params)\n",
    "        mlflow.log_param(\"threshold\", best_threshold)\n",
    "\n",
    "        # Log test metrics\n",
    "        mlflow.log_metrics({\n",
    "            \"Test_MAE\": mae_test,\n",
    "            \"Test_RMSE\": rmse_test,\n",
    "            \"Test_R2\": r2_test\n",
    "        })\n",
    "\n",
    "        # Calculate SHAP values\n",
    "        explainer = shap.Explainer(final_model)\n",
    "        shap_values = explainer(X_train_val)\n",
    "\n",
    "        # Determine the most important feature\n",
    "        shap_importance = np.abs(shap_values.values).mean(axis=0)\n",
    "        most_important_feature = columns_of_interest[np.argmax(shap_importance)]\n",
    "        mlflow.log_param(\"most_important_feature\", most_important_feature)\n",
    "\n",
    "        # Log the model\n",
    "        mlflow.xgboost.log_model(final_model, \"final_model\")\n",
    "\n",
    "        # Plot SHAP summary\n",
    "        shap.summary_plot(shap_values, X_train_val, feature_names=columns_of_interest, show=False)\n",
    "        plt.savefig(\"shap_summary.png\", bbox_inches='tight')\n",
    "        mlflow.log_artifact(\"shap_summary.png\")\n",
    "        plt.close()\n",
    "\n",
    "        # Plot SHAP dependence for the most important feature\n",
    "        shap.dependence_plot(\n",
    "            most_important_feature, shap_values.values, X_train_val,\n",
    "            feature_names=columns_of_interest, show=False\n",
    "        )\n",
    "        plt.savefig(\"shap_dependence.png\", bbox_inches='tight')\n",
    "        mlflow.log_artifact(\"shap_dependence.png\")\n",
    "        plt.close()\n",
    "\n",
    "        # Save SHAP values\n",
    "        shap_values_file = \"shap_values.pkl\"\n",
    "        with open(shap_values_file, \"wb\") as f:\n",
    "            pickle.dump(shap_values, f)\n",
    "        mlflow.log_artifact(shap_values_file)\n",
    "\n",
    "        # Log data information\n",
    "        mlflow.log_param(\"data_path\", path)\n",
    "        mlflow.log_param(\"data_shape\", df.shape)\n",
    "        mlflow.log_param(\"columns_of_interest\", columns_of_interest)\n",
    "        mlflow.log_param(\"train_val_data_shape\", train_val_data.shape)\n",
    "        mlflow.log_param(\"test_data_shape\", test_data.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
