{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"Synthetic_data/experiment_results.csv\"\n",
    "df = pd.read_csv(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iteration_columns = df.filter(regex='^Iteration')\n",
    "# Convertir a numérico\n",
    "iteration_columns = iteration_columns.apply(pd.to_numeric, errors='coerce')\n",
    "# Convertir todos los ceros en NaN\n",
    "iteration_columns = iteration_columns.replace(0, np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate SoH\n",
    "soh = iteration_columns.div(iteration_columns.iloc[:, 0], axis=0)\n",
    "soh_thresholds = pd.DataFrame()\n",
    "# Create columns for the iteration in which SoH reaches 0.98, 0.95, 0.9, 0.85, and 0.8\n",
    "thresholds = [0.98, 0.95, 0.9, 0.85, 0.8]\n",
    "for threshold in thresholds:\n",
    "    soh_thresholds[f'{threshold}'] = soh.apply(lambda row: next((i for i, v in enumerate(row) if v <= threshold), np.nan), axis=1)\n",
    "\n",
    "soh_thresholds.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soh\n",
    "# Create a new dataframe with the specified columns and the SoH thresholds\n",
    "columns_of_interest = ['charge_c_rate_modulation', 'protocol_choice_prob', 'charge_soc_modulation', 'rest_duration_modulation', 'discharge_factor_modulation', 'discharge_soc_modulation']\n",
    "df_with_thresholds = df[columns_of_interest].copy()\n",
    "\n",
    "# Add the SoH thresholds to the dataframe\n",
    "for threshold in thresholds:\n",
    "    df_with_thresholds[f'{threshold}'] = soh_thresholds[f'{threshold}']\n",
    "\n",
    "# Create separate dataframes for each threshold and remove NaN values\n",
    "dfs_by_threshold = {}\n",
    "for threshold in thresholds:\n",
    "    df_threshold = df_with_thresholds[['charge_c_rate_modulation', 'protocol_choice_prob', 'charge_soc_modulation', 'rest_duration_modulation', 'discharge_factor_modulation', 'discharge_soc_modulation', f'{threshold}']].dropna()\n",
    "    dfs_by_threshold[f'{threshold}'] = df_threshold\n",
    "\n",
    "# Example: Access the dataframe for SoH 0.8\n",
    "dfs_by_threshold['0.8']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs_by_threshold.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install xgboost shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "import shap\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "# Initialize a dictionary to store the most important feature, metrics, and SHAP values for each threshold\n",
    "most_important_features = {}\n",
    "metrics = {}\n",
    "shap_values_dict = {}\n",
    "\n",
    "# Loop through each threshold and train an XGBoost model\n",
    "for threshold in thresholds:\n",
    "    # Prepare the data\n",
    "    df_threshold = dfs_by_threshold[f'{threshold}']\n",
    "    X = df_threshold[columns_of_interest]\n",
    "    y = df_threshold[f'{threshold}']\n",
    "    \n",
    "    # Train the XGBoost model\n",
    "    model = xgb.XGBRegressor()\n",
    "    model.fit(X, y)\n",
    "    \n",
    "    # Calculate SHAP values\n",
    "    explainer = shap.Explainer(model)\n",
    "    shap_values = explainer(X)\n",
    "    shap_values_dict[f'{threshold}'] = shap_values\n",
    "    \n",
    "    # Determine the most important feature\n",
    "    shap_importance = np.abs(shap_values.values).mean(axis=0)\n",
    "    most_important_feature = columns_of_interest[np.argmax(shap_importance)]\n",
    "    most_important_features[f'{threshold}'] = most_important_feature\n",
    "    \n",
    "    # Predict the values\n",
    "    y_pred = model.predict(X)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    mae = mean_absolute_error(y, y_pred)\n",
    "    mse = mean_squared_error(y, y_pred)\n",
    "    r2 = r2_score(y, y_pred)\n",
    "    metrics[f'{threshold}'] = {'MAE': mae, 'MSE': mse, 'R²': r2}\n",
    "    \n",
    "    # Print the most important feature and metrics for each threshold\n",
    "    print(f\"The most important feature for SoH {threshold} is {most_important_feature}\")\n",
    "    print(f\"Metrics for SoH {threshold}: MAE = {mae}, MSE = {mse}, R² = {r2}\")\n",
    "    \n",
    "    # Plot the SHAP values for the current threshold\n",
    "    shap.summary_plot(shap_values, X, feature_names=columns_of_interest)\n",
    "    shap.dependence_plot(most_important_feature, shap_values.values, X, feature_names=columns_of_interest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.initjs()\n",
    "shap.force_plot(explainer.expected_value, shap_values.values, X, feature_names=columns_of_interest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install mlflow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import mlflow.xgboost\n",
    "import shap\n",
    "import xgboost as xgb\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import pickle\n",
    "import warnings\n",
    "\n",
    "# Suppress XGBoost model format warning\n",
    "warnings.filterwarnings(action='ignore', category=UserWarning, module='xgboost')\n",
    "\n",
    "# Initialize MLflow experiment\n",
    "mlflow.set_experiment(\"New SoH Prediction Experiment\")\n",
    "\n",
    "# Start an MLflow run\n",
    "with mlflow.start_run():\n",
    "    # Log parameters\n",
    "    mlflow.log_param(\"data_path\", path)\n",
    "    mlflow.log_param(\"thresholds\", thresholds)\n",
    "    \n",
    "    # Initialize a dictionary to store the most important feature, metrics, and SHAP values for each threshold\n",
    "    most_important_features = {}\n",
    "    metrics = {}\n",
    "    shap_values_dict = {}\n",
    "\n",
    "    # Loop through each threshold and train an XGBoost model\n",
    "    for threshold in thresholds:\n",
    "        # Prepare the data\n",
    "        df_threshold = dfs_by_threshold[f'{threshold}']\n",
    "        X = df_threshold[columns_of_interest]\n",
    "        y = df_threshold[f'{threshold}']\n",
    "        \n",
    "        # Train the XGBoost model\n",
    "        model = xgb.XGBRegressor()\n",
    "        model.fit(X, y)\n",
    "        \n",
    "        # Calculate SHAP values\n",
    "        explainer = shap.Explainer(model)\n",
    "        shap_values = explainer(X)\n",
    "        shap_values_dict[f'{threshold}'] = shap_values\n",
    "        \n",
    "        # Determine the most important feature\n",
    "        shap_importance = np.abs(shap_values.values).mean(axis=0)\n",
    "        most_important_feature = columns_of_interest[np.argmax(shap_importance)]\n",
    "        most_important_features[f'{threshold}'] = most_important_feature\n",
    "        \n",
    "        # Predict the values\n",
    "        y_pred = model.predict(X)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        mae = mean_absolute_error(y, y_pred)\n",
    "        mse = mean_squared_error(y, y_pred)\n",
    "        r2 = r2_score(y, y_pred)\n",
    "        metrics[f'{threshold}'] = {'MAE': mae, 'MSE': mse, 'R²': r2}\n",
    "        \n",
    "        # Log metrics\n",
    "        # Log the model with input example\n",
    "        input_example = X.iloc[:5]\n",
    "        mlflow.xgboost.log_model(model, f\"model_{threshold}\", input_example=input_example)\n",
    "        mlflow.log_metric(f\"R2_{threshold}\", r2)\n",
    "        \n",
    "        # Log the model\n",
    "        mlflow.xgboost.log_model(model, f\"model_{threshold}\")\n",
    "        \n",
    "        # Print the most important feature and metrics for each threshold\n",
    "        print(f\"The most important feature for SoH {threshold} is {most_important_feature}\")\n",
    "        print(f\"Metrics for SoH {threshold}: MAE = {mae}, MSE = {mse}, R² = {r2}\")\n",
    "        \n",
    "        # Plot the SHAP values for the current threshold\n",
    "        shap.summary_plot(shap_values, X, feature_names=columns_of_interest)\n",
    "        shap.dependence_plot(most_important_feature, shap_values.values, X, feature_names=columns_of_interest)\n",
    "    \n",
    "    # Log the most important features\n",
    "    mlflow.log_dict(most_important_features, \"most_important_features.json\")\n",
    "    \n",
    "    # Log the SHAP values\n",
    "    for threshold, shap_values in shap_values_dict.items():\n",
    "        shap_values_file = f\"shap_values_{threshold}.pkl\"\n",
    "        with open(shap_values_file, \"wb\") as f:\n",
    "            pickle.dump(shap_values, f)\n",
    "        mlflow.log_artifact(shap_values_file)\n",
    "\n",
    "    # Log the experimental setup\n",
    "    mlflow.log_param(\"experimental_setup\", {\n",
    "        \"data_path\": path,\n",
    "        \"columns_of_interest\": columns_of_interest,\n",
    "        \"thresholds\": thresholds\n",
    "    })\n",
    "\n",
    "    # Log the preprocessing steps\n",
    "    mlflow.log_param(\"preprocessing_steps\", {\n",
    "        \"iteration_columns_conversion\": \"Converted to numeric and replaced zeros with NaN\",\n",
    "        \"soh_calculation\": \"Calculated SoH and created thresholds\",\n",
    "        \"dataframe_creation\": \"Created dataframes for each threshold and removed NaN values\"\n",
    "    })\n",
    "\n",
    "    # Log the model training and evaluation details\n",
    "    mlflow.log_param(\"model_training\", {\n",
    "        \"model_type\": \"XGBoost\",\n",
    "        \"evaluation_metrics\": [\"MAE\", \"MSE\", \"R²\"]\n",
    "    })\n",
    "\n",
    "    # Log the most important features\n",
    "    mlflow.log_dict(most_important_features, \"most_important_features.json\")\n",
    "\n",
    "    # Log the SHAP values\n",
    "    for threshold, shap_values in shap_values_dict.items():\n",
    "        shap_values_file = f\"shap_values_{threshold}.pkl\"\n",
    "        with open(shap_values_file, \"wb\") as f:\n",
    "            pickle.dump(shap_values, f)\n",
    "        mlflow.log_artifact(shap_values_file)\n",
    "\n",
    "    # Track the specific file of the experimental result\n",
    "    # This will log the experimental result file to MLflow\n",
    "    mlflow.log_artifact(path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.end_run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U \"ray[data,train,tune,serve]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import mlflow.xgboost\n",
    "import shap\n",
    "import xgboost as xgb\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import pickle\n",
    "import warnings\n",
    "from ray import tune, train\n",
    "from ray.tune.schedulers import ASHAScheduler\n",
    "\n",
    "# Suppress XGBoost model format warning\n",
    "warnings.filterwarnings(action='ignore', category=UserWarning, module='xgboost')\n",
    "\n",
    "# Initialize MLflow experiment\n",
    "mlflow.set_experiment(\"New SoH Prediction Experiment\")\n",
    "\n",
    "# Define the search space for hyperparameters\n",
    "search_space = {\n",
    "    \"learning_rate\": tune.loguniform(0.01, 0.1),\n",
    "    \"max_depth\": tune.randint(3, 10),\n",
    "    \"min_child_weight\": tune.randint(1, 6),\n",
    "    \"subsample\": tune.uniform(0.5, 1.0),\n",
    "    \"colsample_bytree\": tune.uniform(0.5, 1.0)\n",
    "}\n",
    "\n",
    "# Define the training function\n",
    "def train_model(config, threshold, X, y):\n",
    "    import xgboost as xgb\n",
    "    from sklearn.metrics import mean_absolute_error\n",
    "    # No need to import tune again if already imported globally\n",
    "    model = xgb.XGBRegressor(\n",
    "        learning_rate=config[\"learning_rate\"],\n",
    "        max_depth=config[\"max_depth\"],\n",
    "        min_child_weight=config[\"min_child_weight\"],\n",
    "        subsample=config[\"subsample\"],\n",
    "        colsample_bytree=config[\"colsample_bytree\"]\n",
    "    )\n",
    "    model.fit(X, y)\n",
    "    y_pred = model.predict(X)\n",
    "    mae = mean_absolute_error(y, y_pred)\n",
    "    train.report({\"mae\":mae})\n",
    "\n",
    "# Define or load necessary variables before running the script\n",
    "# For example:\n",
    "# path = \"path_to_your_data.csv\"\n",
    "# thresholds = [0.8, 0.85, 0.9]\n",
    "# columns_of_interest = [\"feature1\", \"feature2\", \"feature3\"]\n",
    "# dfs_by_threshold = {\"0.8\": df1, \"0.85\": df2, \"0.9\": df3}\n",
    "\n",
    "# Start an MLflow run\n",
    "with mlflow.start_run():\n",
    "    # Log parameters\n",
    "    mlflow.log_param(\"data_path\", path)\n",
    "    mlflow.log_param(\"thresholds\", thresholds)\n",
    "    \n",
    "    # Initialize dictionaries to store results\n",
    "    most_important_features = {}\n",
    "    metrics = {}\n",
    "    shap_values_dict = {}\n",
    "\n",
    "    # Loop through each threshold and train an XGBoost model\n",
    "    for threshold in thresholds:\n",
    "        # Prepare the data\n",
    "        df_threshold = dfs_by_threshold[str(threshold)]\n",
    "        X = df_threshold[columns_of_interest]\n",
    "        y = df_threshold[str(threshold)]\n",
    "        \n",
    "        # Perform hyperparameter tuning with Ray Tune\n",
    "        scheduler = ASHAScheduler(\n",
    "            metric=\"mae\",\n",
    "            mode=\"min\",\n",
    "            max_t=10,\n",
    "            grace_period=1,\n",
    "            reduction_factor=2\n",
    "        )\n",
    "        \n",
    "        # Sample a subset of the data to reduce memory usage\n",
    "        X_sample = X.sample(frac=0.5, random_state=42)\n",
    "        y_sample = y.loc[X_sample.index]\n",
    "\n",
    "        analysis = tune.run(\n",
    "            tune.with_parameters(train_model, threshold=threshold, X=X_sample, y=y_sample),\n",
    "            config=search_space,\n",
    "            num_samples=10,\n",
    "            scheduler=scheduler,\n",
    "             max_concurrent_trials=1  # Limit the number of concurrent trials\n",
    "            # For newer versions of Ray, limit concurrency using ConcurrencyLimiter if needed\n",
    "        )\n",
    "        \n",
    "        # Get the best hyperparameters\n",
    "        best_config = analysis.get_best_config(metric=\"mae\", mode=\"min\")\n",
    "        \n",
    "        # Train the XGBoost model with the best hyperparameters\n",
    "        model = xgb.XGBRegressor(\n",
    "            learning_rate=best_config[\"learning_rate\"],\n",
    "            max_depth=best_config[\"max_depth\"],\n",
    "            min_child_weight=best_config[\"min_child_weight\"],\n",
    "            subsample=best_config[\"subsample\"],\n",
    "            colsample_bytree=best_config[\"colsample_bytree\"]\n",
    "        )\n",
    "        model.fit(X, y)\n",
    "        \n",
    "        # Calculate SHAP values\n",
    "        explainer = shap.Explainer(model)\n",
    "        shap_values = explainer(X)\n",
    "        shap_values_dict[str(threshold)] = shap_values\n",
    "        \n",
    "        # Determine the most important feature\n",
    "        shap_importance = np.abs(shap_values.values).mean(axis=0)\n",
    "        most_important_feature = columns_of_interest[np.argmax(shap_importance)]\n",
    "        most_important_features[str(threshold)] = most_important_feature\n",
    "        \n",
    "        # Predict the values\n",
    "        y_pred = model.predict(X)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        mae = mean_absolute_error(y, y_pred)\n",
    "        mse = mean_squared_error(y, y_pred)\n",
    "        r2 = r2_score(y, y_pred)\n",
    "        metrics[str(threshold)] = {'MAE': mae, 'MSE': mse, 'R²': r2}\n",
    "        \n",
    "        # Log metrics\n",
    "        # Log the model with input example\n",
    "        input_example = X.iloc[:5]\n",
    "        mlflow.xgboost.log_model(model, f\"model_{threshold}\", input_example=input_example)\n",
    "        mlflow.log_metric(f\"R2_{threshold}\", r2)\n",
    "        \n",
    "        # Print the most important feature and metrics for each threshold\n",
    "        print(f\"The most important feature for SoH {threshold} is {most_important_feature}\")\n",
    "        print(f\"Metrics for SoH {threshold}: MAE = {mae}, MSE = {mse}, R² = {r2}\")\n",
    "        \n",
    "        # Plot the SHAP values for the current threshold\n",
    "        # Save the plots instead of displaying them\n",
    "        shap.summary_plot(shap_values, X, feature_names=columns_of_interest, show=False)\n",
    "        plt.savefig(f\"shap_summary_{threshold}.png\")\n",
    "        mlflow.log_artifact(f\"shap_summary_{threshold}.png\")\n",
    "        plt.close()\n",
    "\n",
    "        shap.dependence_plot(most_important_feature, shap_values.values, X, feature_names=columns_of_interest, show=False)\n",
    "        plt.savefig(f\"shap_dependence_{threshold}.png\")\n",
    "        mlflow.log_artifact(f\"shap_dependence_{threshold}.png\")\n",
    "        plt.close()\n",
    "    \n",
    "    # Log the most important features\n",
    "    mlflow.log_dict(most_important_features, \"most_important_features.json\")\n",
    "    \n",
    "    # Log the SHAP values\n",
    "    for threshold, shap_values in shap_values_dict.items():\n",
    "        shap_values_file = f\"shap_values_{threshold}.pkl\"\n",
    "        with open(shap_values_file, \"wb\") as f:\n",
    "            pickle.dump(shap_values, f)\n",
    "        mlflow.log_artifact(shap_values_file)\n",
    "\n",
    "    # Log the experimental setup\n",
    "    mlflow.log_param(\"experimental_setup\", {\n",
    "        \"data_path\": path,\n",
    "        \"columns_of_interest\": columns_of_interest,\n",
    "        \"thresholds\": thresholds\n",
    "    })\n",
    "\n",
    "    # Log the preprocessing steps\n",
    "    mlflow.log_param(\"preprocessing_steps\", {\n",
    "        \"iteration_columns_conversion\": \"Converted to numeric and replaced zeros with NaN\",\n",
    "        \"soh_calculation\": \"Calculated SoH and created thresholds\",\n",
    "        \"dataframe_creation\": \"Created dataframes for each threshold and removed NaN values\"\n",
    "    })\n",
    "\n",
    "    # Log the model training and evaluation details\n",
    "    mlflow.log_param(\"model_training\", {\n",
    "        \"model_type\": \"XGBoost\",\n",
    "        \"evaluation_metrics\": [\"MAE\", \"MSE\", \"R²\"]\n",
    "    })\n",
    "\n",
    "    # Track the specific file of the experimental result\n",
    "    # This will log the experimental result file to MLflow\n",
    "    mlflow.log_artifact(path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
